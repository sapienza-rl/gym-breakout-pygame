{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Learn to play at Breakout \n",
    "\n",
    "### Requirements\n",
    "\n",
    "- [install `keras-rl`](https://github.com/keras-rl/keras-rl#installation)\n",
    "\n",
    "      pip install keras-rl\n",
    "      \n",
    "- install the `gym_breakout_pygame` package\n",
    "\n",
    "      pip install gym_breakout_pygame\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 1.9.6\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "from gym_breakout_pygame.wrappers.observation_space import BreakoutN\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import BoltzmannQPolicy\n",
    "from rl.memory import SequentialMemory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0629 23:48:49.540298 140024841986176 deprecation_wrapper.py:119] From /home/marcofavorito/.virtualenvs/gym-breakout-pygame-7UQzWS9l/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0629 23:48:49.551870 140024841986176 deprecation_wrapper.py:119] From /home/marcofavorito/.virtualenvs/gym-breakout-pygame-7UQzWS9l/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0629 23:48:49.561582 140024841986176 deprecation_wrapper.py:119] From /home/marcofavorito/.virtualenvs/gym-breakout-pygame-7UQzWS9l/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                320       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 3)                 195       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 3)                 0         \n",
      "=================================================================\n",
      "Total params: 4,675\n",
      "Trainable params: 4,675\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "env = BreakoutN(encode=False)\n",
    "np.random.seed(123)\n",
    "env.seed(123)\n",
    "nb_actions = env.action_space.n\n",
    "\n",
    "# Next, we build a very simple model.\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(nb_actions))\n",
    "model.add(Activation('linear'))\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 30000 steps ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marcofavorito/.virtualenvs/gym-breakout-pygame-7UQzWS9l/lib/python3.7/site-packages/rl/memory.py:39: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
      "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   419/30000: episode: 1, duration: 3.063s, episode steps: 419, steps per second: 137, episode reward: 15.000, mean reward: 0.036 [0.000, 5.000], mean action: 1.057 [0.000, 2.000], mean observation: 11.356 [0.000, 47.000], loss: 0.023504, mean_absolute_error: 5.422011, mean_q: 8.154562\n",
      "   708/30000: episode: 2, duration: 1.514s, episode steps: 289, steps per second: 191, episode reward: 10.000, mean reward: 0.035 [0.000, 5.000], mean action: 0.931 [0.000, 2.000], mean observation: 11.562 [0.000, 47.000], loss: 0.033845, mean_absolute_error: 5.324571, mean_q: 7.997057\n",
      "   989/30000: episode: 3, duration: 1.480s, episode steps: 281, steps per second: 190, episode reward: 10.000, mean reward: 0.036 [0.000, 5.000], mean action: 0.947 [0.000, 2.000], mean observation: 12.093 [0.000, 47.000], loss: 0.045569, mean_absolute_error: 5.272690, mean_q: 7.907522\n",
      "  1270/30000: episode: 4, duration: 1.508s, episode steps: 281, steps per second: 186, episode reward: 10.000, mean reward: 0.036 [0.000, 5.000], mean action: 1.089 [0.000, 2.000], mean observation: 12.050 [0.000, 47.000], loss: 0.067247, mean_absolute_error: 5.298580, mean_q: 7.945251\n",
      "  1559/30000: episode: 5, duration: 1.528s, episode steps: 289, steps per second: 189, episode reward: 10.000, mean reward: 0.035 [0.000, 5.000], mean action: 1.045 [0.000, 2.000], mean observation: 11.559 [0.000, 47.000], loss: 0.044289, mean_absolute_error: 5.359823, mean_q: 8.047291\n",
      "  1710/30000: episode: 6, duration: 0.839s, episode steps: 151, steps per second: 180, episode reward: 5.000, mean reward: 0.033 [0.000, 5.000], mean action: 0.974 [0.000, 2.000], mean observation: 13.212 [2.000, 47.000], loss: 0.061643, mean_absolute_error: 5.400750, mean_q: 8.091373\n",
      "  1861/30000: episode: 7, duration: 0.800s, episode steps: 151, steps per second: 189, episode reward: 5.000, mean reward: 0.033 [0.000, 5.000], mean action: 0.960 [0.000, 2.000], mean observation: 11.255 [0.000, 47.000], loss: 0.089229, mean_absolute_error: 5.380961, mean_q: 8.065592\n",
      "  2014/30000: episode: 8, duration: 0.903s, episode steps: 153, steps per second: 170, episode reward: 5.000, mean reward: 0.033 [0.000, 5.000], mean action: 1.013 [0.000, 2.000], mean observation: 11.752 [0.000, 47.000], loss: 0.069068, mean_absolute_error: 5.407639, mean_q: 8.112662\n",
      "  2295/30000: episode: 9, duration: 1.581s, episode steps: 281, steps per second: 178, episode reward: 10.000, mean reward: 0.036 [0.000, 5.000], mean action: 1.011 [0.000, 2.000], mean observation: 11.990 [0.000, 47.000], loss: 0.063888, mean_absolute_error: 5.346360, mean_q: 8.005969\n",
      "  2576/30000: episode: 10, duration: 1.493s, episode steps: 281, steps per second: 188, episode reward: 10.000, mean reward: 0.036 [0.000, 5.000], mean action: 1.036 [0.000, 2.000], mean observation: 12.211 [0.000, 47.000], loss: 0.066835, mean_absolute_error: 5.352602, mean_q: 8.014496\n",
      "  3013/30000: episode: 11, duration: 2.456s, episode steps: 437, steps per second: 178, episode reward: 25.000, mean reward: 0.057 [0.000, 5.000], mean action: 1.023 [0.000, 2.000], mean observation: 11.464 [0.000, 47.000], loss: 0.065248, mean_absolute_error: 5.394109, mean_q: 8.082459\n",
      "  3424/30000: episode: 12, duration: 2.527s, episode steps: 411, steps per second: 163, episode reward: 15.000, mean reward: 0.036 [0.000, 5.000], mean action: 0.920 [0.000, 2.000], mean observation: 11.788 [0.000, 47.000], loss: 0.070650, mean_absolute_error: 5.359143, mean_q: 8.032214\n",
      "  3705/30000: episode: 13, duration: 1.647s, episode steps: 281, steps per second: 171, episode reward: 10.000, mean reward: 0.036 [0.000, 5.000], mean action: 0.922 [0.000, 2.000], mean observation: 12.013 [0.000, 47.000], loss: 0.074943, mean_absolute_error: 5.311494, mean_q: 7.961785\n",
      "  4254/30000: episode: 14, duration: 2.972s, episode steps: 549, steps per second: 185, episode reward: 20.000, mean reward: 0.036 [0.000, 5.000], mean action: 0.998 [0.000, 2.000], mean observation: 11.282 [0.000, 47.000], loss: 0.059893, mean_absolute_error: 5.327416, mean_q: 7.985179\n",
      "  4535/30000: episode: 15, duration: 1.516s, episode steps: 281, steps per second: 185, episode reward: 10.000, mean reward: 0.036 [0.000, 5.000], mean action: 0.996 [0.000, 2.000], mean observation: 12.137 [0.000, 47.000], loss: 0.065771, mean_absolute_error: 5.281673, mean_q: 7.919343\n",
      "  5391/30000: episode: 16, duration: 4.622s, episode steps: 856, steps per second: 185, episode reward: 40.000, mean reward: 0.047 [0.000, 5.000], mean action: 0.995 [0.000, 2.000], mean observation: 9.493 [0.000, 47.000], loss: 0.089211, mean_absolute_error: 5.313982, mean_q: 7.977903\n",
      "  5806/30000: episode: 17, duration: 2.248s, episode steps: 415, steps per second: 185, episode reward: 15.000, mean reward: 0.036 [0.000, 5.000], mean action: 1.007 [0.000, 2.000], mean observation: 11.704 [0.000, 47.000], loss: 0.111573, mean_absolute_error: 5.356624, mean_q: 8.039972\n",
      "  6087/30000: episode: 18, duration: 1.611s, episode steps: 281, steps per second: 174, episode reward: 10.000, mean reward: 0.036 [0.000, 5.000], mean action: 0.961 [0.000, 2.000], mean observation: 11.541 [0.000, 47.000], loss: 0.093416, mean_absolute_error: 5.399543, mean_q: 8.095107\n",
      "  6238/30000: episode: 19, duration: 0.827s, episode steps: 151, steps per second: 183, episode reward: 5.000, mean reward: 0.033 [0.000, 5.000], mean action: 0.987 [0.000, 2.000], mean observation: 11.692 [0.000, 47.000], loss: 0.091160, mean_absolute_error: 5.383549, mean_q: 8.082010\n",
      "  6393/30000: episode: 20, duration: 0.881s, episode steps: 155, steps per second: 176, episode reward: 5.000, mean reward: 0.032 [0.000, 5.000], mean action: 1.071 [0.000, 2.000], mean observation: 12.292 [0.000, 47.000], loss: 0.100902, mean_absolute_error: 5.405324, mean_q: 8.110594\n",
      "  6674/30000: episode: 21, duration: 1.526s, episode steps: 281, steps per second: 184, episode reward: 10.000, mean reward: 0.036 [0.000, 5.000], mean action: 0.972 [0.000, 2.000], mean observation: 11.769 [0.000, 47.000], loss: 0.133501, mean_absolute_error: 5.438374, mean_q: 8.152271\n",
      "  6825/30000: episode: 22, duration: 0.822s, episode steps: 151, steps per second: 184, episode reward: 5.000, mean reward: 0.033 [0.000, 5.000], mean action: 0.901 [0.000, 2.000], mean observation: 11.695 [0.000, 47.000], loss: 0.075103, mean_absolute_error: 5.398685, mean_q: 8.102550\n",
      "  7106/30000: episode: 23, duration: 1.507s, episode steps: 281, steps per second: 186, episode reward: 10.000, mean reward: 0.036 [0.000, 5.000], mean action: 1.028 [0.000, 2.000], mean observation: 12.195 [0.000, 47.000], loss: 0.101172, mean_absolute_error: 5.410856, mean_q: 8.113491\n",
      "  7257/30000: episode: 24, duration: 0.824s, episode steps: 151, steps per second: 183, episode reward: 5.000, mean reward: 0.033 [0.000, 5.000], mean action: 1.060 [0.000, 2.000], mean observation: 13.066 [2.000, 47.000], loss: 0.117252, mean_absolute_error: 5.419800, mean_q: 8.125498\n",
      "  7538/30000: episode: 25, duration: 1.538s, episode steps: 281, steps per second: 183, episode reward: 10.000, mean reward: 0.036 [0.000, 5.000], mean action: 0.943 [0.000, 2.000], mean observation: 11.985 [0.000, 47.000], loss: 0.104431, mean_absolute_error: 5.400424, mean_q: 8.104257\n",
      "  7689/30000: episode: 26, duration: 0.827s, episode steps: 151, steps per second: 183, episode reward: 5.000, mean reward: 0.033 [0.000, 5.000], mean action: 1.013 [0.000, 2.000], mean observation: 12.142 [2.000, 47.000], loss: 0.113386, mean_absolute_error: 5.436970, mean_q: 8.136731\n",
      "  7840/30000: episode: 27, duration: 0.809s, episode steps: 151, steps per second: 187, episode reward: 5.000, mean reward: 0.033 [0.000, 5.000], mean action: 1.007 [0.000, 2.000], mean observation: 12.911 [2.000, 47.000], loss: 0.105858, mean_absolute_error: 5.435463, mean_q: 8.152614\n",
      "  8271/30000: episode: 28, duration: 2.332s, episode steps: 431, steps per second: 185, episode reward: 15.000, mean reward: 0.035 [0.000, 5.000], mean action: 0.991 [0.000, 2.000], mean observation: 11.426 [0.000, 47.000], loss: 0.093395, mean_absolute_error: 5.424602, mean_q: 8.135627\n",
      "  8708/30000: episode: 29, duration: 2.518s, episode steps: 437, steps per second: 174, episode reward: 25.000, mean reward: 0.057 [0.000, 5.000], mean action: 1.057 [0.000, 2.000], mean observation: 11.724 [0.000, 47.000], loss: 0.093851, mean_absolute_error: 5.406663, mean_q: 8.102077\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  9265/30000: episode: 30, duration: 3.310s, episode steps: 557, steps per second: 168, episode reward: 20.000, mean reward: 0.036 [0.000, 5.000], mean action: 1.038 [0.000, 2.000], mean observation: 11.119 [0.000, 47.000], loss: 0.073086, mean_absolute_error: 5.475297, mean_q: 8.213145\n",
      "  9676/30000: episode: 31, duration: 2.529s, episode steps: 411, steps per second: 162, episode reward: 15.000, mean reward: 0.036 [0.000, 5.000], mean action: 0.990 [0.000, 2.000], mean observation: 11.747 [0.000, 47.000], loss: 0.092612, mean_absolute_error: 5.488288, mean_q: 8.224232\n",
      "  9965/30000: episode: 32, duration: 1.624s, episode steps: 289, steps per second: 178, episode reward: 10.000, mean reward: 0.035 [0.000, 5.000], mean action: 1.031 [0.000, 2.000], mean observation: 11.562 [0.000, 47.000], loss: 0.087115, mean_absolute_error: 5.484327, mean_q: 8.218536\n",
      " 10246/30000: episode: 33, duration: 1.864s, episode steps: 281, steps per second: 151, episode reward: 10.000, mean reward: 0.036 [0.000, 5.000], mean action: 1.068 [0.000, 2.000], mean observation: 11.874 [0.000, 47.000], loss: 0.062507, mean_absolute_error: 5.473729, mean_q: 8.200583\n",
      " 10665/30000: episode: 34, duration: 2.733s, episode steps: 419, steps per second: 153, episode reward: 15.000, mean reward: 0.036 [0.000, 5.000], mean action: 1.000 [0.000, 2.000], mean observation: 11.518 [0.000, 47.000], loss: 0.076940, mean_absolute_error: 5.480589, mean_q: 8.214701\n",
      " 11076/30000: episode: 35, duration: 2.689s, episode steps: 411, steps per second: 153, episode reward: 15.000, mean reward: 0.036 [0.000, 5.000], mean action: 0.990 [0.000, 2.000], mean observation: 11.282 [0.000, 47.000], loss: 0.078448, mean_absolute_error: 5.509243, mean_q: 8.255825\n",
      " 11357/30000: episode: 36, duration: 1.840s, episode steps: 281, steps per second: 153, episode reward: 10.000, mean reward: 0.036 [0.000, 5.000], mean action: 0.989 [0.000, 2.000], mean observation: 11.695 [0.000, 47.000], loss: 0.062995, mean_absolute_error: 5.494907, mean_q: 8.239648\n",
      " 11508/30000: episode: 37, duration: 0.972s, episode steps: 151, steps per second: 155, episode reward: 5.000, mean reward: 0.033 [0.000, 5.000], mean action: 1.066 [0.000, 2.000], mean observation: 11.712 [0.000, 47.000], loss: 0.097478, mean_absolute_error: 5.502293, mean_q: 8.237699\n",
      " 11797/30000: episode: 38, duration: 1.876s, episode steps: 289, steps per second: 154, episode reward: 10.000, mean reward: 0.035 [0.000, 5.000], mean action: 1.059 [0.000, 2.000], mean observation: 11.209 [0.000, 47.000], loss: 0.085523, mean_absolute_error: 5.502534, mean_q: 8.234865\n",
      " 11948/30000: episode: 39, duration: 0.939s, episode steps: 151, steps per second: 161, episode reward: 5.000, mean reward: 0.033 [0.000, 5.000], mean action: 0.947 [0.000, 2.000], mean observation: 12.076 [0.000, 47.000], loss: 0.069012, mean_absolute_error: 5.469382, mean_q: 8.191037\n",
      " 11969/30000: episode: 40, duration: 0.129s, episode steps: 21, steps per second: 163, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.286 [0.000, 2.000], mean observation: 15.488 [2.000, 47.000], loss: 0.054030, mean_absolute_error: 5.466596, mean_q: 8.201758\n",
      " 12789/30000: episode: 41, duration: 5.276s, episode steps: 820, steps per second: 155, episode reward: 45.000, mean reward: 0.055 [0.000, 5.000], mean action: 0.944 [0.000, 2.000], mean observation: 9.664 [0.000, 46.000], loss: 0.070046, mean_absolute_error: 5.506760, mean_q: 8.251441\n",
      " 13200/30000: episode: 42, duration: 39.199s, episode steps: 411, steps per second: 10, episode reward: 15.000, mean reward: 0.036 [0.000, 5.000], mean action: 1.002 [0.000, 2.000], mean observation: 12.095 [0.000, 47.000], loss: 0.086970, mean_absolute_error: 5.519894, mean_q: 8.264962\n",
      " 13351/30000: episode: 43, duration: 21.518s, episode steps: 151, steps per second: 7, episode reward: 5.000, mean reward: 0.033 [0.000, 5.000], mean action: 1.053 [0.000, 2.000], mean observation: 12.273 [0.000, 47.000], loss: 0.083154, mean_absolute_error: 5.501603, mean_q: 8.242964\n",
      " 13502/30000: episode: 44, duration: 6.769s, episode steps: 151, steps per second: 22, episode reward: 5.000, mean reward: 0.033 [0.000, 5.000], mean action: 0.914 [0.000, 2.000], mean observation: 11.530 [0.000, 47.000], loss: 0.073871, mean_absolute_error: 5.488800, mean_q: 8.216399\n",
      " 13783/30000: episode: 45, duration: 8.805s, episode steps: 281, steps per second: 32, episode reward: 10.000, mean reward: 0.036 [0.000, 5.000], mean action: 1.068 [0.000, 2.000], mean observation: 11.835 [0.000, 47.000], loss: 0.074838, mean_absolute_error: 5.492188, mean_q: 8.228086\n",
      " 13934/30000: episode: 46, duration: 4.376s, episode steps: 151, steps per second: 35, episode reward: 5.000, mean reward: 0.033 [0.000, 5.000], mean action: 1.113 [0.000, 2.000], mean observation: 13.137 [2.000, 47.000], loss: 0.085428, mean_absolute_error: 5.449144, mean_q: 8.137316\n",
      " 14215/30000: episode: 47, duration: 2.760s, episode steps: 281, steps per second: 102, episode reward: 10.000, mean reward: 0.036 [0.000, 5.000], mean action: 1.053 [0.000, 2.000], mean observation: 11.573 [0.000, 47.000], loss: 0.065299, mean_absolute_error: 5.500953, mean_q: 8.241364\n",
      " 14780/30000: episode: 48, duration: 4.648s, episode steps: 565, steps per second: 122, episode reward: 20.000, mean reward: 0.035 [0.000, 5.000], mean action: 0.981 [0.000, 2.000], mean observation: 10.933 [0.000, 47.000], loss: 0.076206, mean_absolute_error: 5.450040, mean_q: 8.158278\n",
      " 15755/30000: episode: 49, duration: 7.649s, episode steps: 975, steps per second: 127, episode reward: 35.000, mean reward: 0.036 [0.000, 5.000], mean action: 1.052 [0.000, 2.000], mean observation: 11.492 [0.000, 47.000], loss: 0.072967, mean_absolute_error: 5.411637, mean_q: 8.106160\n",
      " 16598/30000: episode: 50, duration: 6.474s, episode steps: 843, steps per second: 130, episode reward: 30.000, mean reward: 0.036 [0.000, 5.000], mean action: 1.034 [0.000, 2.000], mean observation: 11.130 [0.000, 47.000], loss: 0.075607, mean_absolute_error: 5.369420, mean_q: 8.035327\n",
      " 16879/30000: episode: 51, duration: 1.947s, episode steps: 281, steps per second: 144, episode reward: 10.000, mean reward: 0.036 [0.000, 5.000], mean action: 0.989 [0.000, 2.000], mean observation: 11.671 [0.000, 47.000], loss: 0.072962, mean_absolute_error: 5.353611, mean_q: 8.033890\n",
      " 17160/30000: episode: 52, duration: 1.980s, episode steps: 281, steps per second: 142, episode reward: 10.000, mean reward: 0.036 [0.000, 5.000], mean action: 0.979 [0.000, 2.000], mean observation: 12.173 [0.000, 47.000], loss: 0.080256, mean_absolute_error: 5.372193, mean_q: 8.052150\n",
      " 18254/30000: episode: 53, duration: 7.704s, episode steps: 1094, steps per second: 142, episode reward: 45.000, mean reward: 0.041 [0.000, 5.000], mean action: 0.984 [0.000, 2.000], mean observation: 9.058 [0.000, 45.000], loss: 0.069729, mean_absolute_error: 5.378454, mean_q: 8.060588\n",
      " 18405/30000: episode: 54, duration: 0.979s, episode steps: 151, steps per second: 154, episode reward: 5.000, mean reward: 0.033 [0.000, 5.000], mean action: 1.040 [0.000, 2.000], mean observation: 12.758 [2.000, 47.000], loss: 0.100240, mean_absolute_error: 5.441647, mean_q: 8.140321\n",
      " 18556/30000: episode: 55, duration: 0.988s, episode steps: 151, steps per second: 153, episode reward: 5.000, mean reward: 0.033 [0.000, 5.000], mean action: 1.046 [0.000, 2.000], mean observation: 12.851 [2.000, 47.000], loss: 0.064240, mean_absolute_error: 5.396173, mean_q: 8.078123\n",
      " 18967/30000: episode: 56, duration: 2.716s, episode steps: 411, steps per second: 151, episode reward: 15.000, mean reward: 0.036 [0.000, 5.000], mean action: 1.017 [0.000, 2.000], mean observation: 10.870 [0.000, 47.000], loss: 0.066144, mean_absolute_error: 5.390724, mean_q: 8.068348\n",
      " 19516/30000: episode: 57, duration: 3.556s, episode steps: 549, steps per second: 154, episode reward: 20.000, mean reward: 0.036 [0.000, 5.000], mean action: 1.018 [0.000, 2.000], mean observation: 11.185 [0.000, 47.000], loss: 0.066537, mean_absolute_error: 5.349319, mean_q: 8.006159\n",
      " 19667/30000: episode: 58, duration: 1.062s, episode steps: 151, steps per second: 142, episode reward: 5.000, mean reward: 0.033 [0.000, 5.000], mean action: 0.960 [0.000, 2.000], mean observation: 12.323 [0.000, 47.000], loss: 0.053429, mean_absolute_error: 5.301332, mean_q: 7.932405\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 19818/30000: episode: 59, duration: 1.007s, episode steps: 151, steps per second: 150, episode reward: 5.000, mean reward: 0.033 [0.000, 5.000], mean action: 1.046 [0.000, 2.000], mean observation: 12.611 [2.000, 47.000], loss: 0.048499, mean_absolute_error: 5.274741, mean_q: 7.892972\n",
      " 20099/30000: episode: 60, duration: 1.833s, episode steps: 281, steps per second: 153, episode reward: 10.000, mean reward: 0.036 [0.000, 5.000], mean action: 0.989 [0.000, 2.000], mean observation: 11.076 [0.000, 47.000], loss: 0.083863, mean_absolute_error: 5.255237, mean_q: 7.854093\n",
      " 20510/30000: episode: 61, duration: 2.638s, episode steps: 411, steps per second: 156, episode reward: 15.000, mean reward: 0.036 [0.000, 5.000], mean action: 0.971 [0.000, 2.000], mean observation: 11.533 [0.000, 47.000], loss: 0.081554, mean_absolute_error: 5.245347, mean_q: 7.848889\n",
      " 20661/30000: episode: 62, duration: 0.980s, episode steps: 151, steps per second: 154, episode reward: 5.000, mean reward: 0.033 [0.000, 5.000], mean action: 0.980 [0.000, 2.000], mean observation: 13.237 [2.000, 47.000], loss: 0.084919, mean_absolute_error: 5.171615, mean_q: 7.734274\n",
      " 21201/30000: episode: 63, duration: 3.181s, episode steps: 540, steps per second: 170, episode reward: 40.000, mean reward: 0.074 [0.000, 5.000], mean action: 1.052 [0.000, 2.000], mean observation: 10.177 [0.000, 47.000], loss: 0.073890, mean_absolute_error: 5.178969, mean_q: 7.753498\n",
      " 21620/30000: episode: 64, duration: 2.491s, episode steps: 419, steps per second: 168, episode reward: 15.000, mean reward: 0.036 [0.000, 5.000], mean action: 1.064 [0.000, 2.000], mean observation: 11.783 [0.000, 47.000], loss: 0.068657, mean_absolute_error: 5.183169, mean_q: 7.760361\n",
      " 21903/30000: episode: 65, duration: 1.623s, episode steps: 283, steps per second: 174, episode reward: 10.000, mean reward: 0.035 [0.000, 5.000], mean action: 1.011 [0.000, 2.000], mean observation: 11.348 [0.000, 47.000], loss: 0.068112, mean_absolute_error: 5.172075, mean_q: 7.739052\n",
      " 22192/30000: episode: 66, duration: 1.678s, episode steps: 289, steps per second: 172, episode reward: 10.000, mean reward: 0.035 [0.000, 5.000], mean action: 0.997 [0.000, 2.000], mean observation: 11.823 [0.000, 47.000], loss: 0.061409, mean_absolute_error: 5.151805, mean_q: 7.710713\n",
      " 23013/30000: episode: 67, duration: 4.888s, episode steps: 821, steps per second: 168, episode reward: 45.000, mean reward: 0.055 [0.000, 5.000], mean action: 1.018 [0.000, 2.000], mean observation: 10.449 [0.000, 45.000], loss: 0.078283, mean_absolute_error: 5.175792, mean_q: 7.753218\n",
      " 23170/30000: episode: 68, duration: 0.868s, episode steps: 157, steps per second: 181, episode reward: 5.000, mean reward: 0.032 [0.000, 5.000], mean action: 1.032 [0.000, 2.000], mean observation: 11.451 [0.000, 47.000], loss: 0.081257, mean_absolute_error: 5.208488, mean_q: 7.807643\n",
      " 23461/30000: episode: 69, duration: 1.639s, episode steps: 291, steps per second: 178, episode reward: 10.000, mean reward: 0.034 [0.000, 5.000], mean action: 0.983 [0.000, 2.000], mean observation: 11.838 [0.000, 47.000], loss: 0.069605, mean_absolute_error: 5.179669, mean_q: 7.758339\n",
      " 23612/30000: episode: 70, duration: 0.924s, episode steps: 151, steps per second: 163, episode reward: 5.000, mean reward: 0.033 [0.000, 5.000], mean action: 1.099 [0.000, 2.000], mean observation: 13.005 [2.000, 47.000], loss: 0.056289, mean_absolute_error: 5.161196, mean_q: 7.728524\n",
      " 23763/30000: episode: 71, duration: 0.832s, episode steps: 151, steps per second: 181, episode reward: 5.000, mean reward: 0.033 [0.000, 5.000], mean action: 0.914 [0.000, 2.000], mean observation: 11.949 [0.000, 47.000], loss: 0.058138, mean_absolute_error: 5.164929, mean_q: 7.737612\n",
      " 24312/30000: episode: 72, duration: 3.278s, episode steps: 549, steps per second: 167, episode reward: 20.000, mean reward: 0.036 [0.000, 5.000], mean action: 1.053 [0.000, 2.000], mean observation: 11.626 [0.000, 47.000], loss: 0.076413, mean_absolute_error: 5.182180, mean_q: 7.753909\n",
      " 24593/30000: episode: 73, duration: 1.589s, episode steps: 281, steps per second: 177, episode reward: 10.000, mean reward: 0.036 [0.000, 5.000], mean action: 0.986 [0.000, 2.000], mean observation: 12.334 [0.000, 47.000], loss: 0.065890, mean_absolute_error: 5.139128, mean_q: 7.694658\n",
      " 24874/30000: episode: 74, duration: 1.603s, episode steps: 281, steps per second: 175, episode reward: 10.000, mean reward: 0.036 [0.000, 5.000], mean action: 0.996 [0.000, 2.000], mean observation: 12.069 [0.000, 47.000], loss: 0.073072, mean_absolute_error: 5.125919, mean_q: 7.676718\n",
      " 25155/30000: episode: 75, duration: 1.633s, episode steps: 281, steps per second: 172, episode reward: 10.000, mean reward: 0.036 [0.000, 5.000], mean action: 1.011 [0.000, 2.000], mean observation: 11.702 [0.000, 47.000], loss: 0.068206, mean_absolute_error: 5.113692, mean_q: 7.652061\n",
      " 25566/30000: episode: 76, duration: 2.262s, episode steps: 411, steps per second: 182, episode reward: 15.000, mean reward: 0.036 [0.000, 5.000], mean action: 0.966 [0.000, 2.000], mean observation: 11.151 [0.000, 47.000], loss: 0.077724, mean_absolute_error: 5.100288, mean_q: 7.631071\n",
      " 25717/30000: episode: 77, duration: 0.848s, episode steps: 151, steps per second: 178, episode reward: 5.000, mean reward: 0.033 [0.000, 5.000], mean action: 1.119 [0.000, 2.000], mean observation: 12.624 [1.000, 47.000], loss: 0.077024, mean_absolute_error: 5.094176, mean_q: 7.623677\n",
      " 25868/30000: episode: 78, duration: 0.886s, episode steps: 151, steps per second: 170, episode reward: 5.000, mean reward: 0.033 [0.000, 5.000], mean action: 1.086 [0.000, 2.000], mean observation: 12.975 [2.000, 47.000], loss: 0.064595, mean_absolute_error: 5.051201, mean_q: 7.568133\n",
      " 26800/30000: episode: 79, duration: 5.062s, episode steps: 932, steps per second: 184, episode reward: 45.000, mean reward: 0.048 [0.000, 5.000], mean action: 0.994 [0.000, 2.000], mean observation: 10.709 [0.000, 45.000], loss: 0.060297, mean_absolute_error: 5.043658, mean_q: 7.552600\n",
      " 26821/30000: episode: 80, duration: 0.163s, episode steps: 21, steps per second: 129, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.905 [0.000, 2.000], mean observation: 15.667 [3.000, 47.000], loss: 0.083568, mean_absolute_error: 5.025069, mean_q: 7.508173\n",
      " 26972/30000: episode: 81, duration: 0.968s, episode steps: 151, steps per second: 156, episode reward: 5.000, mean reward: 0.033 [0.000, 5.000], mean action: 1.079 [0.000, 2.000], mean observation: 13.190 [2.000, 47.000], loss: 0.085163, mean_absolute_error: 5.017543, mean_q: 7.503658\n",
      " 26993/30000: episode: 82, duration: 0.115s, episode steps: 21, steps per second: 182, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.143 [0.000, 2.000], mean observation: 15.286 [0.000, 47.000], loss: 0.036328, mean_absolute_error: 4.991973, mean_q: 7.482440\n",
      " 27144/30000: episode: 83, duration: 0.840s, episode steps: 151, steps per second: 180, episode reward: 5.000, mean reward: 0.033 [0.000, 5.000], mean action: 1.126 [0.000, 2.000], mean observation: 12.382 [0.000, 47.000], loss: 0.075987, mean_absolute_error: 5.019235, mean_q: 7.514573\n",
      " 27841/30000: episode: 84, duration: 3.916s, episode steps: 697, steps per second: 178, episode reward: 25.000, mean reward: 0.036 [0.000, 5.000], mean action: 1.019 [0.000, 2.000], mean observation: 11.388 [0.000, 47.000], loss: 0.070356, mean_absolute_error: 4.973818, mean_q: 7.444518\n",
      " 27992/30000: episode: 85, duration: 0.847s, episode steps: 151, steps per second: 178, episode reward: 5.000, mean reward: 0.033 [0.000, 5.000], mean action: 1.099 [0.000, 2.000], mean observation: 12.040 [0.000, 47.000], loss: 0.059368, mean_absolute_error: 4.957906, mean_q: 7.421698\n",
      " 28691/30000: episode: 86, duration: 3.865s, episode steps: 699, steps per second: 181, episode reward: 25.000, mean reward: 0.036 [0.000, 5.000], mean action: 0.994 [0.000, 2.000], mean observation: 11.509 [0.000, 47.000], loss: 0.067174, mean_absolute_error: 4.937042, mean_q: 7.392046\n",
      " 29600/30000: episode: 87, duration: 5.029s, episode steps: 909, steps per second: 181, episode reward: 30.000, mean reward: 0.033 [0.000, 5.000], mean action: 0.927 [0.000, 2.000], mean observation: 10.336 [0.000, 47.000], loss: 0.057695, mean_absolute_error: 4.890296, mean_q: 7.319613\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 29751/30000: episode: 88, duration: 0.921s, episode steps: 151, steps per second: 164, episode reward: 5.000, mean reward: 0.033 [0.000, 5.000], mean action: 0.980 [0.000, 2.000], mean observation: 12.175 [0.000, 47.000], loss: 0.060603, mean_absolute_error: 4.842140, mean_q: 7.244157\n",
      "done, took 257.579 seconds\n",
      "Testing for 5 episodes ...\n"
     ]
    }
   ],
   "source": [
    "# Configure and compile the RL agent\n",
    "memory = SequentialMemory(limit=50000, window_length=1)\n",
    "policy = BoltzmannQPolicy()\n",
    "dqn = DQNAgent(model=model, nb_actions=nb_actions, memory=memory, nb_steps_warmup=10,\n",
    "               target_model_update=1e-2, policy=policy)\n",
    "dqn.compile(Adam(lr=1e-3), metrics=['mae'])\n",
    "\n",
    "# learn\n",
    "dqn.fit(env, nb_steps=30000, visualize=False, verbose=2)\n",
    "\n",
    "# save \n",
    "dqn.save_weights('dqn_{}_weights.h5f'.format(\"breakout-n\"), overwrite=True)\n",
    "\n",
    "# Finally, evaluate our algorithm for 5 episodes.\n",
    "dqn.test(env, nb_episodes=5, visualize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
