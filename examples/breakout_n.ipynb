{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Learn to play at Breakout \n",
    "\n",
    "### Requirements\n",
    "\n",
    "- In the repo root directory, do `pipenv install --dev` \n",
    "- Or, install the needed packages:\n",
    "\n",
    "      pip install keras-rl gym_breakout_pygame keras tensorflow-cpu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "from gym.wrappers import Monitor\n",
    "from gym_breakout_pygame.wrappers.observation_space import BreakoutN\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import BoltzmannQPolicy\n",
    "from rl.memory import SequentialMemory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nflatten_3 (Flatten)          (None, 4)                 0         \n_________________________________________________________________\ndense_7 (Dense)              (None, 64)                320       \n_________________________________________________________________\nactivation_7 (Activation)    (None, 64)                0         \n_________________________________________________________________\ndense_8 (Dense)              (None, 64)                4160      \n_________________________________________________________________\nactivation_8 (Activation)    (None, 64)                0         \n_________________________________________________________________\ndense_9 (Dense)              (None, 3)                 195       \n_________________________________________________________________\nactivation_9 (Activation)    (None, 3)                 0         \n=================================================================\nTotal params: 4,675\nTrainable params: 4,675\nNon-trainable params: 0\n_________________________________________________________________\nNone\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "env = BreakoutN(encode=False)\n",
    "np.random.seed(123)\n",
    "env.seed(123)\n",
    "nb_actions = env.action_space.n\n",
    "\n",
    "# Next, we build a very simple model.\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(nb_actions))\n",
    "model.add(Activation('linear'))\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Training for 30000 steps ...\n",
      "   151/30000: episode: 1, duration: 1.470s, episode steps: 151, steps per second: 103, episode reward: 5.000, mean reward: 0.033 [0.000, 5.000], mean action: 0.325 [0.000, 1.000], mean observation: 12.275 [0.000, 47.000], loss: 0.479828, mean_absolute_error: 2.318425, mean_q: 1.703398\n   172/30000: episode: 2, duration: 0.116s, episode steps: 21, steps per second: 180, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.476 [0.000, 1.000], mean observation: 15.274 [1.000, 47.000], loss: 0.487880, mean_absolute_error: 2.015612, mean_q: 0.596119\n",
      "   193/30000: episode: 3, duration: 0.116s, episode steps: 21, steps per second: 181, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.857 [0.000, 1.000], mean observation: 15.024 [0.000, 47.000], loss: 0.435630, mean_absolute_error: 2.082524, mean_q: 0.867428\n   214/30000: episode: 4, duration: 0.112s, episode steps: 21, steps per second: 187, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.524 [0.000, 1.000], mean observation: 15.607 [1.000, 47.000], loss: 0.191307, mean_absolute_error: 1.983096, mean_q: 1.071521\n",
      "   235/30000: episode: 5, duration: 0.114s, episode steps: 21, steps per second: 184, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.667 [0.000, 1.000], mean observation: 15.321 [0.000, 47.000], loss: 0.152663, mean_absolute_error: 1.868621, mean_q: 0.609894\n   256/30000: episode: 6, duration: 0.111s, episode steps: 21, steps per second: 189, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.476 [0.000, 1.000], mean observation: 15.464 [1.000, 47.000], loss: 0.082131, mean_absolute_error: 1.858058, mean_q: 0.656099\n",
      "   277/30000: episode: 7, duration: 0.116s, episode steps: 21, steps per second: 181, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.524 [0.000, 1.000], mean observation: 15.500 [1.000, 47.000], loss: 0.090197, mean_absolute_error: 1.873260, mean_q: 0.747166\n   298/30000: episode: 8, duration: 0.116s, episode steps: 21, steps per second: 181, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.476 [0.000, 1.000], mean observation: 15.417 [1.000, 47.000], loss: 0.018315, mean_absolute_error: 1.883731, mean_q: 0.781522\n",
      "   319/30000: episode: 9, duration: 0.126s, episode steps: 21, steps per second: 167, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.667 [0.000, 1.000], mean observation: 15.298 [0.000, 47.000], loss: 0.059765, mean_absolute_error: 1.896396, mean_q: 0.774248\n   340/30000: episode: 10, duration: 0.112s, episode steps: 21, steps per second: 187, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.381 [0.000, 1.000], mean observation: 15.714 [2.000, 47.000], loss: 0.038599, mean_absolute_error: 1.917137, mean_q: 0.779965\n",
      "   361/30000: episode: 11, duration: 0.122s, episode steps: 21, steps per second: 172, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.524 [0.000, 1.000], mean observation: 15.464 [1.000, 47.000], loss: 0.044970, mean_absolute_error: 1.931533, mean_q: 0.761013\n   382/30000: episode: 12, duration: 0.129s, episode steps: 21, steps per second: 163, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.571 [0.000, 1.000], mean observation: 15.369 [0.000, 47.000], loss: 0.065198, mean_absolute_error: 1.964512, mean_q: 0.773655\n",
      "   403/30000: episode: 13, duration: 0.123s, episode steps: 21, steps per second: 171, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.524 [0.000, 1.000], mean observation: 15.512 [1.000, 47.000], loss: 0.028287, mean_absolute_error: 1.952814, mean_q: 0.784250\n   424/30000: episode: 14, duration: 0.124s, episode steps: 21, steps per second: 170, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.571 [0.000, 1.000], mean observation: 15.250 [0.000, 47.000], loss: 0.026247, mean_absolute_error: 1.966576, mean_q: 0.758203\n",
      "   445/30000: episode: 15, duration: 0.123s, episode steps: 21, steps per second: 170, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.381 [0.000, 1.000], mean observation: 15.619 [2.000, 47.000], loss: 0.024978, mean_absolute_error: 1.963621, mean_q: 0.762850\n   466/30000: episode: 16, duration: 0.116s, episode steps: 21, steps per second: 182, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.667 [0.000, 1.000], mean observation: 15.214 [0.000, 47.000], loss: 0.072031, mean_absolute_error: 1.981208, mean_q: 0.738902\n",
      "   487/30000: episode: 17, duration: 0.118s, episode steps: 21, steps per second: 178, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.524 [0.000, 1.000], mean observation: 15.476 [1.000, 47.000], loss: 0.013690, mean_absolute_error: 1.976053, mean_q: 0.760812\n   508/30000: episode: 18, duration: 0.114s, episode steps: 21, steps per second: 184, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.619 [0.000, 1.000], mean observation: 15.381 [0.000, 47.000], loss: 0.037504, mean_absolute_error: 1.960569, mean_q: 0.765546\n",
      "   529/30000: episode: 19, duration: 0.112s, episode steps: 21, steps per second: 188, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.429 [0.000, 1.000], mean observation: 15.643 [2.000, 47.000], loss: 0.037558, mean_absolute_error: 1.968823, mean_q: 0.751433\n   550/30000: episode: 20, duration: 0.128s, episode steps: 21, steps per second: 165, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.429 [0.000, 1.000], mean observation: 15.381 [2.000, 47.000], loss: 0.025191, mean_absolute_error: 1.977141, mean_q: 0.726331\n",
      "   571/30000: episode: 21, duration: 0.115s, episode steps: 21, steps per second: 183, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.286 [0.000, 1.000], mean observation: 15.607 [3.000, 47.000], loss: 0.026538, mean_absolute_error: 1.962988, mean_q: 0.741834\n   592/30000: episode: 22, duration: 0.120s, episode steps: 21, steps per second: 175, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.619 [0.000, 1.000], mean observation: 15.298 [0.000, 47.000], loss: 0.014616, mean_absolute_error: 1.973116, mean_q: 0.700936\n",
      "   613/30000: episode: 23, duration: 0.129s, episode steps: 21, steps per second: 163, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.571 [0.000, 1.000], mean observation: 15.381 [0.000, 47.000], loss: 0.020046, mean_absolute_error: 1.970835, mean_q: 0.740530\n   634/30000: episode: 24, duration: 0.130s, episode steps: 21, steps per second: 161, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.476 [0.000, 1.000], mean observation: 15.321 [1.000, 47.000], loss: 0.056642, mean_absolute_error: 1.999496, mean_q: 0.722967\n",
      "   655/30000: episode: 25, duration: 0.127s, episode steps: 21, steps per second: 165, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.571 [0.000, 1.000], mean observation: 15.321 [0.000, 47.000], loss: 0.023790, mean_absolute_error: 1.991496, mean_q: 0.754882\n   676/30000: episode: 26, duration: 0.117s, episode steps: 21, steps per second: 180, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.381 [0.000, 1.000], mean observation: 15.595 [2.000, 47.000], loss: 0.031574, mean_absolute_error: 2.009439, mean_q: 0.750426\n",
      "   697/30000: episode: 27, duration: 0.120s, episode steps: 21, steps per second: 175, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.381 [0.000, 1.000], mean observation: 15.655 [2.000, 47.000], loss: 0.021904, mean_absolute_error: 2.010164, mean_q: 0.738005\n   718/30000: episode: 28, duration: 0.126s, episode steps: 21, steps per second: 167, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.429 [0.000, 1.000], mean observation: 15.607 [2.000, 47.000], loss: 0.012259, mean_absolute_error: 2.026718, mean_q: 0.696573\n",
      "   739/30000: episode: 29, duration: 0.122s, episode steps: 21, steps per second: 172, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.714 [0.000, 1.000], mean observation: 15.119 [0.000, 47.000], loss: 0.019601, mean_absolute_error: 2.043272, mean_q: 0.727436\n",
      "  1020/30000: episode: 30, duration: 1.514s, episode steps: 281, steps per second: 186, episode reward: 10.000, mean reward: 0.036 [0.000, 5.000], mean action: 0.644 [0.000, 2.000], mean observation: 10.512 [0.000, 47.000], loss: 0.152584, mean_absolute_error: 1.447044, mean_q: 0.484683\n",
      "  1171/30000: episode: 31, duration: 0.851s, episode steps: 151, steps per second: 177, episode reward: 5.000, mean reward: 0.033 [0.000, 5.000], mean action: 0.894 [0.000, 2.000], mean observation: 12.833 [2.000, 47.000], loss: 0.093349, mean_absolute_error: 0.713877, mean_q: -0.478251\n",
      "  1322/30000: episode: 32, duration: 0.924s, episode steps: 151, steps per second: 163, episode reward: 5.000, mean reward: 0.033 [0.000, 5.000], mean action: 1.079 [0.000, 2.000], mean observation: 12.305 [0.000, 47.000], loss: 0.033913, mean_absolute_error: 0.580769, mean_q: -0.250084\n",
      "  1603/30000: episode: 33, duration: 1.595s, episode steps: 281, steps per second: 176, episode reward: 10.000, mean reward: 0.036 [0.000, 5.000], mean action: 1.000 [0.000, 2.000], mean observation: 11.620 [0.000, 47.000], loss: 0.044556, mean_absolute_error: 0.562692, mean_q: -0.019592\n",
      "  2014/30000: episode: 34, duration: 2.527s, episode steps: 411, steps per second: 163, episode reward: 15.000, mean reward: 0.036 [0.000, 5.000], mean action: 1.066 [0.000, 2.000], mean observation: 12.290 [0.000, 47.000], loss: 0.031625, mean_absolute_error: 0.568948, mean_q: 0.233337\n",
      "  2165/30000: episode: 35, duration: 0.859s, episode steps: 151, steps per second: 176, episode reward: 5.000, mean reward: 0.033 [0.000, 5.000], mean action: 1.040 [0.000, 2.000], mean observation: 13.023 [2.000, 47.000], loss: 0.025527, mean_absolute_error: 0.620034, mean_q: 0.482353\n",
      "  2446/30000: episode: 36, duration: 1.659s, episode steps: 281, steps per second: 169, episode reward: 10.000, mean reward: 0.036 [0.000, 5.000], mean action: 1.032 [0.000, 2.000], mean observation: 12.423 [0.000, 47.000], loss: 0.027056, mean_absolute_error: 0.676766, mean_q: 0.705680\n",
      "  2865/30000: episode: 37, duration: 2.433s, episode steps: 419, steps per second: 172, episode reward: 15.000, mean reward: 0.036 [0.000, 5.000], mean action: 1.045 [0.000, 2.000], mean observation: 11.946 [0.000, 47.000], loss: 0.036707, mean_absolute_error: 0.786302, mean_q: 1.029979\n",
      "  3434/30000: episode: 38, duration: 3.475s, episode steps: 569, steps per second: 164, episode reward: 30.000, mean reward: 0.053 [0.000, 5.000], mean action: 1.065 [0.000, 2.000], mean observation: 11.725 [0.000, 47.000], loss: 0.042913, mean_absolute_error: 1.013571, mean_q: 1.490463\n",
      "  3585/30000: episode: 39, duration: 0.852s, episode steps: 151, steps per second: 177, episode reward: 5.000, mean reward: 0.033 [0.000, 5.000], mean action: 1.013 [0.000, 2.000], mean observation: 12.743 [2.000, 47.000], loss: 0.049325, mean_absolute_error: 1.237382, mean_q: 1.888913\n",
      "  3736/30000: episode: 40, duration: 0.907s, episode steps: 151, steps per second: 166, episode reward: 5.000, mean reward: 0.033 [0.000, 5.000], mean action: 1.060 [0.000, 2.000], mean observation: 13.157 [2.000, 47.000], loss: 0.049849, mean_absolute_error: 1.331561, mean_q: 2.028390\n",
      "  4017/30000: episode: 41, duration: 1.607s, episode steps: 281, steps per second: 175, episode reward: 10.000, mean reward: 0.036 [0.000, 5.000], mean action: 0.993 [0.000, 2.000], mean observation: 12.153 [0.000, 47.000], loss: 0.045382, mean_absolute_error: 1.461483, mean_q: 2.246186\n",
      "  4168/30000: episode: 42, duration: 1.005s, episode steps: 151, steps per second: 150, episode reward: 5.000, mean reward: 0.033 [0.000, 5.000], mean action: 1.166 [0.000, 2.000], mean observation: 13.182 [2.000, 47.000], loss: 0.048017, mean_absolute_error: 1.635148, mean_q: 2.525466\n",
      "  4319/30000: episode: 43, duration: 1.015s, episode steps: 151, steps per second: 149, episode reward: 5.000, mean reward: 0.033 [0.000, 5.000], mean action: 1.119 [0.000, 2.000], mean observation: 13.017 [2.000, 47.000], loss: 0.050606, mean_absolute_error: 1.743581, mean_q: 2.689887\n",
      "  4470/30000: episode: 44, duration: 0.929s, episode steps: 151, steps per second: 162, episode reward: 5.000, mean reward: 0.033 [0.000, 5.000], mean action: 1.007 [0.000, 2.000], mean observation: 13.434 [2.000, 47.000], loss: 0.051406, mean_absolute_error: 1.896902, mean_q: 2.913613\n",
      "  4621/30000: episode: 45, duration: 0.944s, episode steps: 151, steps per second: 160, episode reward: 5.000, mean reward: 0.033 [0.000, 5.000], mean action: 1.060 [0.000, 2.000], mean observation: 12.219 [0.000, 47.000], loss: 0.057428, mean_absolute_error: 2.020612, mean_q: 3.112905\n",
      "  4772/30000: episode: 46, duration: 0.996s, episode steps: 151, steps per second: 152, episode reward: 5.000, mean reward: 0.033 [0.000, 5.000], mean action: 1.086 [0.000, 2.000], mean observation: 13.449 [2.000, 47.000], loss: 0.084066, mean_absolute_error: 2.163200, mean_q: 3.338119\n",
      "  5183/30000: episode: 47, duration: 2.239s, episode steps: 411, steps per second: 184, episode reward: 15.000, mean reward: 0.036 [0.000, 5.000], mean action: 1.056 [0.000, 2.000], mean observation: 12.432 [0.000, 47.000], loss: 0.063446, mean_absolute_error: 2.409118, mean_q: 3.710862\n",
      "  5334/30000: episode: 48, duration: 0.856s, episode steps: 151, steps per second: 176, episode reward: 5.000, mean reward: 0.033 [0.000, 5.000], mean action: 1.040 [0.000, 2.000], mean observation: 13.406 [2.000, 47.000], loss: 0.068193, mean_absolute_error: 2.711595, mean_q: 4.171000\n  5355/30000: episode: 49, duration: 0.147s, episode steps: 21, steps per second: 143, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.238 [0.000, 2.000], mean observation: 16.583 [1.000, 47.000], loss: 0.070669, mean_absolute_error: 2.810310, mean_q: 4.348316\n",
      "  5506/30000: episode: 50, duration: 0.831s, episode steps: 151, steps per second: 182, episode reward: 5.000, mean reward: 0.033 [0.000, 5.000], mean action: 1.099 [0.000, 2.000], mean observation: 13.152 [2.000, 47.000], loss: 0.080224, mean_absolute_error: 2.889171, mean_q: 4.431395\n",
      "  5657/30000: episode: 51, duration: 0.833s, episode steps: 151, steps per second: 181, episode reward: 5.000, mean reward: 0.033 [0.000, 5.000], mean action: 1.086 [0.000, 2.000], mean observation: 13.086 [2.000, 47.000], loss: 0.092358, mean_absolute_error: 3.049867, mean_q: 4.681522\n",
      "  5808/30000: episode: 52, duration: 0.879s, episode steps: 151, steps per second: 172, episode reward: 5.000, mean reward: 0.033 [0.000, 5.000], mean action: 1.219 [0.000, 2.000], mean observation: 12.454 [0.000, 47.000], loss: 0.091925, mean_absolute_error: 3.237631, mean_q: 4.964385\n",
      "  6233/30000: episode: 53, duration: 2.299s, episode steps: 425, steps per second: 185, episode reward: 15.000, mean reward: 0.035 [0.000, 5.000], mean action: 1.052 [0.000, 2.000], mean observation: 12.035 [0.000, 47.000], loss: 0.095946, mean_absolute_error: 3.574905, mean_q: 5.491817\n",
      "  6384/30000: episode: 54, duration: 0.936s, episode steps: 151, steps per second: 161, episode reward: 5.000, mean reward: 0.033 [0.000, 5.000], mean action: 1.199 [0.000, 2.000], mean observation: 12.507 [0.000, 47.000], loss: 0.105237, mean_absolute_error: 3.962503, mean_q: 6.111590\n",
      "  6535/30000: episode: 55, duration: 0.837s, episode steps: 151, steps per second: 180, episode reward: 5.000, mean reward: 0.033 [0.000, 5.000], mean action: 1.113 [0.000, 2.000], mean observation: 13.171 [2.000, 47.000], loss: 0.146701, mean_absolute_error: 4.185269, mean_q: 6.423102\n",
      "  6686/30000: episode: 56, duration: 0.831s, episode steps: 151, steps per second: 182, episode reward: 5.000, mean reward: 0.033 [0.000, 5.000], mean action: 1.020 [0.000, 2.000], mean observation: 13.020 [2.000, 47.000], loss: 0.116854, mean_absolute_error: 4.267230, mean_q: 6.553692\n",
      "  7791/30000: episode: 57, duration: 5.979s, episode steps: 1105, steps per second: 185, episode reward: 45.000, mean reward: 0.041 [0.000, 5.000], mean action: 1.082 [0.000, 2.000], mean observation: 11.388 [0.000, 47.000], loss: 0.177127, mean_absolute_error: 5.005449, mean_q: 7.638265\n",
      "  8072/30000: episode: 58, duration: 6.374s, episode steps: 281, steps per second: 44, episode reward: 10.000, mean reward: 0.036 [0.000, 5.000], mean action: 1.050 [0.000, 2.000], mean observation: 12.354 [0.000, 47.000], loss: 0.226696, mean_absolute_error: 5.692528, mean_q: 8.657461\n",
      "  8223/30000: episode: 59, duration: 3.286s, episode steps: 151, steps per second: 46, episode reward: 5.000, mean reward: 0.033 [0.000, 5.000], mean action: 1.093 [0.000, 2.000], mean observation: 13.257 [2.000, 47.000], loss: 0.241879, mean_absolute_error: 5.899218, mean_q: 8.976258\n",
      "  8634/30000: episode: 60, duration: 8.981s, episode steps: 411, steps per second: 46, episode reward: 15.000, mean reward: 0.036 [0.000, 5.000], mean action: 1.073 [0.000, 2.000], mean observation: 12.444 [0.000, 47.000], loss: 0.244957, mean_absolute_error: 6.036972, mean_q: 9.150363\n",
      "  8785/30000: episode: 61, duration: 3.148s, episode steps: 151, steps per second: 48, episode reward: 5.000, mean reward: 0.033 [0.000, 5.000], mean action: 1.000 [0.000, 2.000], mean observation: 12.934 [2.000, 47.000], loss: 0.271222, mean_absolute_error: 6.162698, mean_q: 9.340050\n",
      "  8936/30000: episode: 62, duration: 3.191s, episode steps: 151, steps per second: 47, episode reward: 5.000, mean reward: 0.033 [0.000, 5.000], mean action: 0.960 [0.000, 2.000], mean observation: 12.430 [0.000, 47.000], loss: 0.235541, mean_absolute_error: 6.169534, mean_q: 9.353970\n",
      "  9087/30000: episode: 63, duration: 3.305s, episode steps: 151, steps per second: 46, episode reward: 5.000, mean reward: 0.033 [0.000, 5.000], mean action: 1.040 [0.000, 2.000], mean observation: 12.344 [0.000, 47.000], loss: 0.291273, mean_absolute_error: 6.423487, mean_q: 9.730296\n",
      "  9238/30000: episode: 64, duration: 3.773s, episode steps: 151, steps per second: 40, episode reward: 5.000, mean reward: 0.033 [0.000, 5.000], mean action: 1.099 [0.000, 2.000], mean observation: 12.980 [2.000, 47.000], loss: 0.276417, mean_absolute_error: 6.352807, mean_q: 9.616243\n",
      "  9533/30000: episode: 65, duration: 9.099s, episode steps: 295, steps per second: 32, episode reward: 10.000, mean reward: 0.034 [0.000, 5.000], mean action: 1.122 [0.000, 2.000], mean observation: 12.515 [0.000, 47.000], loss: 0.277787, mean_absolute_error: 6.471378, mean_q: 9.777703\n",
      " 10246/30000: episode: 66, duration: 25.092s, episode steps: 713, steps per second: 28, episode reward: 35.000, mean reward: 0.049 [0.000, 5.000], mean action: 1.034 [0.000, 2.000], mean observation: 11.613 [0.000, 47.000], loss: 0.289943, mean_absolute_error: 6.751845, mean_q: 10.189805\n",
      " 10527/30000: episode: 67, duration: 4.817s, episode steps: 281, steps per second: 58, episode reward: 10.000, mean reward: 0.036 [0.000, 5.000], mean action: 1.096 [0.000, 2.000], mean observation: 11.827 [0.000, 47.000], loss: 0.392859, mean_absolute_error: 6.863020, mean_q: 10.337067\n",
      " 10678/30000: episode: 68, duration: 1.687s, episode steps: 151, steps per second: 90, episode reward: 5.000, mean reward: 0.033 [0.000, 5.000], mean action: 1.159 [0.000, 2.000], mean observation: 13.291 [2.000, 47.000], loss: 0.354009, mean_absolute_error: 6.914150, mean_q: 10.412833\n",
      " 11628/30000: episode: 69, duration: 10.459s, episode steps: 950, steps per second: 91, episode reward: 45.000, mean reward: 0.047 [0.000, 5.000], mean action: 1.056 [0.000, 2.000], mean observation: 11.315 [0.000, 47.000], loss: 0.326501, mean_absolute_error: 6.930792, mean_q: 10.421635\n",
      " 11779/30000: episode: 70, duration: 1.502s, episode steps: 151, steps per second: 101, episode reward: 5.000, mean reward: 0.033 [0.000, 5.000], mean action: 1.040 [0.000, 2.000], mean observation: 12.306 [0.000, 47.000], loss: 0.334554, mean_absolute_error: 6.992325, mean_q: 10.512082\n",
      " 11930/30000: episode: 71, duration: 1.501s, episode steps: 151, steps per second: 101, episode reward: 5.000, mean reward: 0.033 [0.000, 5.000], mean action: 1.066 [0.000, 2.000], mean observation: 12.397 [0.000, 47.000], loss: 0.375407, mean_absolute_error: 7.004040, mean_q: 10.539400\n",
      " 12081/30000: episode: 72, duration: 1.406s, episode steps: 151, steps per second: 107, episode reward: 5.000, mean reward: 0.033 [0.000, 5.000], mean action: 1.040 [0.000, 2.000], mean observation: 12.710 [2.000, 47.000], loss: 0.335667, mean_absolute_error: 7.091883, mean_q: 10.682296\n",
      " 12362/30000: episode: 73, duration: 2.687s, episode steps: 281, steps per second: 105, episode reward: 10.000, mean reward: 0.036 [0.000, 5.000], mean action: 1.039 [0.000, 2.000], mean observation: 12.584 [0.000, 47.000], loss: 0.327441, mean_absolute_error: 7.065262, mean_q: 10.619469\n",
      " 12773/30000: episode: 74, duration: 4.137s, episode steps: 411, steps per second: 99, episode reward: 15.000, mean reward: 0.036 [0.000, 5.000], mean action: 1.046 [0.000, 2.000], mean observation: 12.259 [0.000, 47.000], loss: 0.305306, mean_absolute_error: 7.162008, mean_q: 10.781713\n",
      " 13054/30000: episode: 75, duration: 2.436s, episode steps: 281, steps per second: 115, episode reward: 10.000, mean reward: 0.036 [0.000, 5.000], mean action: 1.103 [0.000, 2.000], mean observation: 12.202 [0.000, 47.000], loss: 0.278148, mean_absolute_error: 7.237905, mean_q: 10.909343\n",
      " 13205/30000: episode: 76, duration: 1.289s, episode steps: 151, steps per second: 117, episode reward: 5.000, mean reward: 0.033 [0.000, 5.000], mean action: 1.079 [0.000, 2.000], mean observation: 12.041 [0.000, 47.000], loss: 0.307513, mean_absolute_error: 7.252892, mean_q: 10.910885\n",
      " 13356/30000: episode: 77, duration: 1.410s, episode steps: 151, steps per second: 107, episode reward: 5.000, mean reward: 0.033 [0.000, 5.000], mean action: 0.940 [0.000, 2.000], mean observation: 12.401 [2.000, 47.000], loss: 0.291935, mean_absolute_error: 7.346283, mean_q: 11.052340\n",
      " 13507/30000: episode: 78, duration: 1.382s, episode steps: 151, steps per second: 109, episode reward: 5.000, mean reward: 0.033 [0.000, 5.000], mean action: 1.073 [0.000, 2.000], mean observation: 13.295 [2.000, 47.000], loss: 0.238687, mean_absolute_error: 7.329851, mean_q: 11.028318\n",
      " 13788/30000: episode: 79, duration: 2.833s, episode steps: 281, steps per second: 99, episode reward: 10.000, mean reward: 0.036 [0.000, 5.000], mean action: 1.000 [0.000, 2.000], mean observation: 11.617 [0.000, 47.000], loss: 0.321830, mean_absolute_error: 7.349491, mean_q: 11.049000\n",
      " 14069/30000: episode: 80, duration: 2.617s, episode steps: 281, steps per second: 107, episode reward: 10.000, mean reward: 0.036 [0.000, 5.000], mean action: 1.057 [0.000, 2.000], mean observation: 11.740 [0.000, 47.000], loss: 0.285486, mean_absolute_error: 7.302924, mean_q: 10.985092\n",
      " 14618/30000: episode: 81, duration: 5.244s, episode steps: 549, steps per second: 105, episode reward: 20.000, mean reward: 0.036 [0.000, 5.000], mean action: 1.042 [0.000, 2.000], mean observation: 12.126 [0.000, 47.000], loss: 0.266311, mean_absolute_error: 7.333820, mean_q: 11.034887\n",
      " 15029/30000: episode: 82, duration: 3.306s, episode steps: 411, steps per second: 124, episode reward: 15.000, mean reward: 0.036 [0.000, 5.000], mean action: 1.054 [0.000, 2.000], mean observation: 12.272 [0.000, 47.000], loss: 0.275962, mean_absolute_error: 7.380917, mean_q: 11.098067\n",
      " 15180/30000: episode: 83, duration: 1.197s, episode steps: 151, steps per second: 126, episode reward: 5.000, mean reward: 0.033 [0.000, 5.000], mean action: 1.179 [0.000, 2.000], mean observation: 13.308 [2.000, 47.000], loss: 0.242346, mean_absolute_error: 7.408224, mean_q: 11.152772\n",
      " 15461/30000: episode: 84, duration: 2.283s, episode steps: 281, steps per second: 123, episode reward: 10.000, mean reward: 0.036 [0.000, 5.000], mean action: 0.957 [0.000, 2.000], mean observation: 12.207 [0.000, 47.000], loss: 0.249226, mean_absolute_error: 7.403720, mean_q: 11.129328\n",
      " 15742/30000: episode: 85, duration: 2.134s, episode steps: 281, steps per second: 132, episode reward: 10.000, mean reward: 0.036 [0.000, 5.000], mean action: 0.940 [0.000, 2.000], mean observation: 11.836 [0.000, 47.000], loss: 0.269953, mean_absolute_error: 7.411638, mean_q: 11.129509\n",
      " 15893/30000: episode: 86, duration: 1.169s, episode steps: 151, steps per second: 129, episode reward: 5.000, mean reward: 0.033 [0.000, 5.000], mean action: 1.086 [0.000, 2.000], mean observation: 12.760 [2.000, 47.000], loss: 0.195722, mean_absolute_error: 7.352054, mean_q: 11.051885\n",
      " 16174/30000: episode: 87, duration: 2.036s, episode steps: 281, steps per second: 138, episode reward: 10.000, mean reward: 0.036 [0.000, 5.000], mean action: 1.125 [0.000, 2.000], mean observation: 12.036 [0.000, 47.000], loss: 0.244277, mean_absolute_error: 7.392756, mean_q: 11.121307\n",
      " 16325/30000: episode: 88, duration: 1.119s, episode steps: 151, steps per second: 135, episode reward: 5.000, mean reward: 0.033 [0.000, 5.000], mean action: 1.086 [0.000, 2.000], mean observation: 13.406 [2.000, 47.000], loss: 0.206099, mean_absolute_error: 7.421659, mean_q: 11.158852\n",
      " 16606/30000: episode: 89, duration: 2.066s, episode steps: 281, steps per second: 136, episode reward: 10.000, mean reward: 0.036 [0.000, 5.000], mean action: 1.068 [0.000, 2.000], mean observation: 12.213 [0.000, 47.000], loss: 0.263669, mean_absolute_error: 7.435047, mean_q: 11.161113\n",
      " 16887/30000: episode: 90, duration: 2.036s, episode steps: 281, steps per second: 138, episode reward: 10.000, mean reward: 0.036 [0.000, 5.000], mean action: 1.043 [0.000, 2.000], mean observation: 11.718 [0.000, 47.000], loss: 0.246834, mean_absolute_error: 7.434573, mean_q: 11.162215\n",
      " 17168/30000: episode: 91, duration: 2.157s, episode steps: 281, steps per second: 130, episode reward: 10.000, mean reward: 0.036 [0.000, 5.000], mean action: 1.096 [0.000, 2.000], mean observation: 12.247 [0.000, 47.000], loss: 0.229716, mean_absolute_error: 7.441288, mean_q: 11.147044\n",
      " 17579/30000: episode: 92, duration: 2.965s, episode steps: 411, steps per second: 139, episode reward: 15.000, mean reward: 0.036 [0.000, 5.000], mean action: 1.027 [0.000, 2.000], mean observation: 12.317 [0.000, 47.000], loss: 0.206048, mean_absolute_error: 7.406745, mean_q: 11.099450\n",
      " 17860/30000: episode: 93, duration: 2.012s, episode steps: 281, steps per second: 140, episode reward: 10.000, mean reward: 0.036 [0.000, 5.000], mean action: 1.036 [0.000, 2.000], mean observation: 12.072 [0.000, 47.000], loss: 0.210860, mean_absolute_error: 7.417013, mean_q: 11.132252\n",
      " 18705/30000: episode: 94, duration: 5.970s, episode steps: 845, steps per second: 142, episode reward: 30.000, mean reward: 0.036 [0.000, 5.000], mean action: 0.988 [0.000, 2.000], mean observation: 11.055 [0.000, 47.000], loss: 0.225704, mean_absolute_error: 7.383301, mean_q: 11.059733\n",
      " 19897/30000: episode: 95, duration: 7.298s, episode steps: 1192, steps per second: 163, episode reward: 45.000, mean reward: 0.038 [0.000, 5.000], mean action: 0.999 [0.000, 2.000], mean observation: 11.155 [0.000, 47.000], loss: 0.177231, mean_absolute_error: 7.163953, mean_q: 10.723739\n",
      " 20048/30000: episode: 96, duration: 0.832s, episode steps: 151, steps per second: 181, episode reward: 5.000, mean reward: 0.033 [0.000, 5.000], mean action: 1.020 [0.000, 2.000], mean observation: 13.161 [2.000, 47.000], loss: 0.174044, mean_absolute_error: 7.056639, mean_q: 10.560275\n",
      " 20199/30000: episode: 97, duration: 0.813s, episode steps: 151, steps per second: 186, episode reward: 5.000, mean reward: 0.033 [0.000, 5.000], mean action: 1.026 [0.000, 2.000], mean observation: 12.094 [0.000, 47.000], loss: 0.171016, mean_absolute_error: 7.052440, mean_q: 10.546433\n",
      " 20929/30000: episode: 98, duration: 3.869s, episode steps: 730, steps per second: 189, episode reward: 35.000, mean reward: 0.048 [0.000, 5.000], mean action: 1.026 [0.000, 2.000], mean observation: 9.665 [0.000, 47.000], loss: 0.161334, mean_absolute_error: 6.968295, mean_q: 10.422845\n",
      " 21210/30000: episode: 99, duration: 1.592s, episode steps: 281, steps per second: 176, episode reward: 10.000, mean reward: 0.036 [0.000, 5.000], mean action: 1.036 [0.000, 2.000], mean observation: 12.579 [0.000, 47.000], loss: 0.171728, mean_absolute_error: 6.909341, mean_q: 10.329296\n",
      " 21361/30000: episode: 100, duration: 0.829s, episode steps: 151, steps per second: 182, episode reward: 5.000, mean reward: 0.033 [0.000, 5.000], mean action: 0.921 [0.000, 2.000], mean observation: 12.988 [2.000, 47.000], loss: 0.164339, mean_absolute_error: 6.808726, mean_q: 10.180441\n",
      " 21650/30000: episode: 101, duration: 1.509s, episode steps: 289, steps per second: 192, episode reward: 10.000, mean reward: 0.035 [0.000, 5.000], mean action: 0.952 [0.000, 2.000], mean observation: 11.247 [0.000, 47.000], loss: 0.137108, mean_absolute_error: 6.806921, mean_q: 10.184025\n",
      " 21931/30000: episode: 102, duration: 1.469s, episode steps: 281, steps per second: 191, episode reward: 10.000, mean reward: 0.036 [0.000, 5.000], mean action: 1.011 [0.000, 2.000], mean observation: 11.739 [0.000, 47.000], loss: 0.145870, mean_absolute_error: 6.732029, mean_q: 10.068334\n",
      " 22216/30000: episode: 103, duration: 1.516s, episode steps: 285, steps per second: 188, episode reward: 10.000, mean reward: 0.035 [0.000, 5.000], mean action: 0.986 [0.000, 2.000], mean observation: 11.625 [0.000, 47.000], loss: 0.151618, mean_absolute_error: 6.683099, mean_q: 10.000327\n",
      " 22635/30000: episode: 104, duration: 2.274s, episode steps: 419, steps per second: 184, episode reward: 15.000, mean reward: 0.036 [0.000, 5.000], mean action: 1.029 [0.000, 2.000], mean observation: 11.148 [0.000, 47.000], loss: 0.139685, mean_absolute_error: 6.636292, mean_q: 9.931478\n",
      " 23050/30000: episode: 105, duration: 2.265s, episode steps: 415, steps per second: 183, episode reward: 15.000, mean reward: 0.036 [0.000, 5.000], mean action: 0.969 [0.000, 2.000], mean observation: 11.873 [0.000, 47.000], loss: 0.138529, mean_absolute_error: 6.581522, mean_q: 9.857800\n",
      " 23345/30000: episode: 106, duration: 1.654s, episode steps: 295, steps per second: 178, episode reward: 10.000, mean reward: 0.034 [0.000, 5.000], mean action: 1.044 [0.000, 2.000], mean observation: 11.600 [0.000, 47.000], loss: 0.184911, mean_absolute_error: 6.497403, mean_q: 9.707663\n",
      " 23496/30000: episode: 107, duration: 0.774s, episode steps: 151, steps per second: 195, episode reward: 5.000, mean reward: 0.033 [0.000, 5.000], mean action: 1.119 [0.000, 2.000], mean observation: 12.983 [2.000, 47.000], loss: 0.150767, mean_absolute_error: 6.495268, mean_q: 9.730314\n",
      " 23907/30000: episode: 108, duration: 2.228s, episode steps: 411, steps per second: 184, episode reward: 15.000, mean reward: 0.036 [0.000, 5.000], mean action: 1.024 [0.000, 2.000], mean observation: 12.356 [0.000, 47.000], loss: 0.132087, mean_absolute_error: 6.455789, mean_q: 9.670238\n",
      " 24318/30000: episode: 109, duration: 2.193s, episode steps: 411, steps per second: 187, episode reward: 15.000, mean reward: 0.036 [0.000, 5.000], mean action: 1.027 [0.000, 2.000], mean observation: 12.266 [0.000, 47.000], loss: 0.130129, mean_absolute_error: 6.384118, mean_q: 9.552741\n",
      " 24607/30000: episode: 110, duration: 1.554s, episode steps: 289, steps per second: 186, episode reward: 10.000, mean reward: 0.035 [0.000, 5.000], mean action: 0.882 [0.000, 2.000], mean observation: 11.454 [0.000, 47.000], loss: 0.147203, mean_absolute_error: 6.350885, mean_q: 9.491140\n",
      " 25911/30000: episode: 111, duration: 6.750s, episode steps: 1304, steps per second: 193, episode reward: 40.000, mean reward: 0.031 [0.000, 5.000], mean action: 0.983 [0.000, 2.000], mean observation: 10.598 [0.000, 47.000], loss: 0.107479, mean_absolute_error: 6.217959, mean_q: 9.307325\n",
      " 26062/30000: episode: 112, duration: 0.798s, episode steps: 151, steps per second: 189, episode reward: 5.000, mean reward: 0.033 [0.000, 5.000], mean action: 1.079 [0.000, 2.000], mean observation: 13.366 [2.000, 47.000], loss: 0.146762, mean_absolute_error: 6.108101, mean_q: 9.146043\n",
      " 26359/30000: episode: 113, duration: 1.589s, episode steps: 297, steps per second: 187, episode reward: 20.000, mean reward: 0.067 [0.000, 5.000], mean action: 1.074 [0.000, 2.000], mean observation: 11.662 [0.000, 47.000], loss: 0.126284, mean_absolute_error: 6.117665, mean_q: 9.161258\n",
      " 26640/30000: episode: 114, duration: 1.480s, episode steps: 281, steps per second: 190, episode reward: 10.000, mean reward: 0.036 [0.000, 5.000], mean action: 0.957 [0.000, 2.000], mean observation: 11.340 [0.000, 47.000], loss: 0.107396, mean_absolute_error: 6.048542, mean_q: 9.064609\n",
      " 26921/30000: episode: 115, duration: 1.472s, episode steps: 281, steps per second: 191, episode reward: 10.000, mean reward: 0.036 [0.000, 5.000], mean action: 0.993 [0.000, 2.000], mean observation: 11.345 [0.000, 47.000], loss: 0.137831, mean_absolute_error: 6.050159, mean_q: 9.073187\n",
      " 27344/30000: episode: 116, duration: 2.188s, episode steps: 423, steps per second: 193, episode reward: 15.000, mean reward: 0.035 [0.000, 5.000], mean action: 0.983 [0.000, 2.000], mean observation: 11.525 [0.000, 47.000], loss: 0.110596, mean_absolute_error: 6.030264, mean_q: 9.026215\n",
      " 27625/30000: episode: 117, duration: 1.532s, episode steps: 281, steps per second: 183, episode reward: 10.000, mean reward: 0.036 [0.000, 5.000], mean action: 0.922 [0.000, 2.000], mean observation: 10.883 [0.000, 47.000], loss: 0.108481, mean_absolute_error: 5.946068, mean_q: 8.895966\n",
      " 27910/30000: episode: 118, duration: 1.555s, episode steps: 285, steps per second: 183, episode reward: 10.000, mean reward: 0.035 [0.000, 5.000], mean action: 1.000 [0.000, 2.000], mean observation: 12.105 [0.000, 47.000], loss: 0.118421, mean_absolute_error: 5.885719, mean_q: 8.805964\n",
      " 28475/30000: episode: 119, duration: 2.902s, episode steps: 565, steps per second: 195, episode reward: 20.000, mean reward: 0.035 [0.000, 5.000], mean action: 1.012 [0.000, 2.000], mean observation: 11.445 [0.000, 47.000], loss: 0.105468, mean_absolute_error: 5.817904, mean_q: 8.710806\n",
      " 28626/30000: episode: 120, duration: 0.821s, episode steps: 151, steps per second: 184, episode reward: 5.000, mean reward: 0.033 [0.000, 5.000], mean action: 0.934 [0.000, 2.000], mean observation: 11.957 [0.000, 47.000], loss: 0.111762, mean_absolute_error: 5.795010, mean_q: 8.665671\n",
      " 28777/30000: episode: 121, duration: 0.866s, episode steps: 151, steps per second: 174, episode reward: 5.000, mean reward: 0.033 [0.000, 5.000], mean action: 0.980 [0.000, 2.000], mean observation: 12.697 [2.000, 47.000], loss: 0.075268, mean_absolute_error: 5.751266, mean_q: 8.608932\n",
      " 29066/30000: episode: 122, duration: 1.538s, episode steps: 289, steps per second: 188, episode reward: 10.000, mean reward: 0.035 [0.000, 5.000], mean action: 0.958 [0.000, 2.000], mean observation: 11.844 [0.000, 47.000], loss: 0.120383, mean_absolute_error: 5.739104, mean_q: 8.586999\n",
      "done, took 249.023 seconds\n"
     ],
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": [
      "/home/marcofavorito/.virtualenvs/gym-breakout-pygame-7UQzWS9l/lib/python3.7/site-packages/rl/memory.py:39: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# Configure and compile the RL agent\n",
    "memory = SequentialMemory(limit=50000, window_length=1)\n",
    "policy = BoltzmannQPolicy()\n",
    "dqn = DQNAgent(model=model, nb_actions=nb_actions, memory=memory, nb_steps_warmup=10,\n",
    "               target_model_update=1e-2, policy=policy)\n",
    "dqn.compile(Adam(lr=1e-3), metrics=['mae'])\n",
    "\n",
    "# learn\n",
    "dqn.fit(env, nb_steps=30000, visualize=False, verbose=2)\n",
    "\n",
    "# save \n",
    "dqn.save_weights('dqn_{}_weights.h5f'.format(\"breakout-n\"), overwrite=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Testing for 5 episodes ...\n",
      "Episode 1: reward: 45.000, steps: 1819\n",
      "Episode 2: reward: 45.000, steps: 1819\n",
      "Episode 3: reward: 45.000, steps: 1819\n",
      "Episode 4: reward: 45.000, steps: 1819\n",
      "Episode 5: reward: 45.000, steps: 1819\n"
     ],
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": [
      "/home/marcofavorito/.virtualenvs/gym-breakout-pygame-7UQzWS9l/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Trying to monitor an environment which has no 'spec' set. This usually means you did not create it via 'gym.make', and is recommended only for advanced users.\u001b[0m\n  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ],
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "<keras.callbacks.History at 0x7fa6382a82e8>"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 19
    }
   ],
   "source": [
    "\n",
    "# Evaluate for 5 episodes.\n",
    "dqn.test(Monitor(env, \".\", force=True), nb_episodes=5, visualize=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "Now check the `examples/` folder, you should be able to see the recordings of the learned policy.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n",
     "is_executing": false
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}