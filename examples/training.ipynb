{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Learn to play at Breakout \n",
    "\n",
    "### Requirements\n",
    "\n",
    "- In the repo root directory, do `pipenv install --dev` \n",
    "- Or, install the needed packages:\n",
    "\n",
    "      pip install keras-rl gym_breakout_pygame keras tensorflow-cpu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%import numpy as np\n"
    }
   },
   "outputs": [],
   "source": [
    "from gym.wrappers import Monitor\n",
    "from gym_breakout_pygame.wrappers.normal_space import BreakoutNMultiDiscrete\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.policy import BoltzmannQPolicy\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Model: \"sequential_2\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nflatten_2 (Flatten)          (None, 4)                 0         \n_________________________________________________________________\ndense_4 (Dense)              (None, 64)                320       \n_________________________________________________________________\nactivation_4 (Activation)    (None, 64)                0         \n_________________________________________________________________\ndense_5 (Dense)              (None, 64)                4160      \n_________________________________________________________________\nactivation_5 (Activation)    (None, 64)                0         \n_________________________________________________________________\ndense_6 (Dense)              (None, 3)                 195       \n_________________________________________________________________\nactivation_6 (Activation)    (None, 3)                 0         \n=================================================================\nTotal params: 4,675\nTrainable params: 4,675\nNon-trainable params: 0\n_________________________________________________________________\nNone\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "env = BreakoutNMultiDiscrete()\n",
    "np.random.seed(123)\n",
    "env.seed(123)\n",
    "nb_actions = env.action_space.n\n",
    "\n",
    "# Next, we build a very simple model.\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(nb_actions))\n",
    "model.add(Activation('linear'))\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Training for 50000 steps ...\n",
      "   151/50000: episode: 1, duration: 1.793s, episode steps: 151, steps per second: 84, episode reward: 5.000, mean reward: 0.033 [0.000, 5.000], mean action: 1.073 [0.000, 2.000], mean observation: 12.233 [2.000, 47.000], loss: 0.127331, mae: 0.433560, mean_q: -0.177764\n   172/50000: episode: 2, duration: 0.151s, episode steps: 21, steps per second: 139, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.952 [0.000, 2.000], mean observation: 15.452 [2.000, 47.000], loss: 0.060645, mae: 0.358177, mean_q: -0.180976\n",
      "   453/50000: episode: 3, duration: 1.962s, episode steps: 281, steps per second: 143, episode reward: 10.000, mean reward: 0.036 [0.000, 5.000], mean action: 1.078 [0.000, 2.000], mean observation: 11.617 [0.000, 47.000], loss: 0.054258, mae: 0.315772, mean_q: -0.111330\n",
      "   604/50000: episode: 4, duration: 1.099s, episode steps: 151, steps per second: 137, episode reward: 5.000, mean reward: 0.033 [0.000, 5.000], mean action: 1.093 [0.000, 2.000], mean observation: 11.697 [0.000, 47.000], loss: 0.039685, mae: 0.357132, mean_q: -0.092351\n",
      "   755/50000: episode: 5, duration: 1.091s, episode steps: 151, steps per second: 138, episode reward: 5.000, mean reward: 0.033 [0.000, 5.000], mean action: 1.119 [0.000, 2.000], mean observation: 13.368 [2.000, 47.000], loss: 0.046153, mae: 0.391754, mean_q: 0.078534\n",
      "  1038/50000: episode: 6, duration: 2.051s, episode steps: 283, steps per second: 138, episode reward: 10.000, mean reward: 0.035 [0.000, 5.000], mean action: 0.965 [0.000, 2.000], mean observation: 11.179 [0.000, 47.000], loss: 0.044170, mae: 0.431753, mean_q: 0.237489\n",
      "  1195/50000: episode: 7, duration: 1.222s, episode steps: 157, steps per second: 129, episode reward: 5.000, mean reward: 0.032 [0.000, 5.000], mean action: 1.025 [0.000, 2.000], mean observation: 12.317 [0.000, 47.000], loss: 0.043309, mae: 0.469655, mean_q: 0.357498\n",
      "  1346/50000: episode: 8, duration: 1.153s, episode steps: 151, steps per second: 131, episode reward: 5.000, mean reward: 0.033 [0.000, 5.000], mean action: 1.033 [0.000, 2.000], mean observation: 12.205 [0.000, 47.000], loss: 0.038523, mae: 0.514018, mean_q: 0.468983\n",
      "  2171/50000: episode: 9, duration: 5.733s, episode steps: 825, steps per second: 144, episode reward: 30.000, mean reward: 0.036 [0.000, 5.000], mean action: 1.033 [0.000, 2.000], mean observation: 11.648 [0.000, 47.000], loss: 0.041031, mae: 0.645296, mean_q: 0.777093\n",
      "  2322/50000: episode: 10, duration: 1.105s, episode steps: 151, steps per second: 137, episode reward: 5.000, mean reward: 0.033 [0.000, 5.000], mean action: 1.053 [0.000, 2.000], mean observation: 13.325 [2.000, 47.000], loss: 0.035152, mae: 0.842428, mean_q: 1.196550\n",
      "  2605/50000: episode: 11, duration: 2.258s, episode steps: 283, steps per second: 125, episode reward: 10.000, mean reward: 0.035 [0.000, 5.000], mean action: 1.004 [0.000, 2.000], mean observation: 11.559 [0.000, 47.000], loss: 0.044926, mae: 1.003476, mean_q: 1.473567\n",
      "  2756/50000: episode: 12, duration: 1.369s, episode steps: 151, steps per second: 110, episode reward: 5.000, mean reward: 0.033 [0.000, 5.000], mean action: 1.026 [0.000, 2.000], mean observation: 12.440 [0.000, 47.000], loss: 0.045127, mae: 1.101156, mean_q: 1.649332\n",
      "  2909/50000: episode: 13, duration: 1.097s, episode steps: 153, steps per second: 139, episode reward: 5.000, mean reward: 0.033 [0.000, 5.000], mean action: 1.020 [0.000, 2.000], mean observation: 12.359 [0.000, 47.000], loss: 0.038492, mae: 1.152471, mean_q: 1.749512\n",
      "  3066/50000: episode: 14, duration: 1.184s, episode steps: 157, steps per second: 133, episode reward: 5.000, mean reward: 0.032 [0.000, 5.000], mean action: 1.121 [0.000, 2.000], mean observation: 12.592 [0.000, 47.000], loss: 0.046505, mae: 1.224037, mean_q: 1.860774\n",
      "  3217/50000: episode: 15, duration: 1.103s, episode steps: 151, steps per second: 137, episode reward: 5.000, mean reward: 0.033 [0.000, 5.000], mean action: 1.086 [0.000, 2.000], mean observation: 13.267 [2.000, 47.000], loss: 0.043733, mae: 1.365310, mean_q: 2.085753\n",
      "  3368/50000: episode: 16, duration: 1.130s, episode steps: 151, steps per second: 134, episode reward: 5.000, mean reward: 0.033 [0.000, 5.000], mean action: 1.139 [0.000, 2.000], mean observation: 13.187 [2.000, 47.000], loss: 0.050428, mae: 1.465278, mean_q: 2.241736\n",
      "  3787/50000: episode: 17, duration: 2.904s, episode steps: 419, steps per second: 144, episode reward: 15.000, mean reward: 0.036 [0.000, 5.000], mean action: 1.057 [0.000, 2.000], mean observation: 11.745 [0.000, 47.000], loss: 0.050275, mae: 1.680409, mean_q: 2.579248\n",
      "  4210/50000: episode: 18, duration: 2.964s, episode steps: 423, steps per second: 143, episode reward: 15.000, mean reward: 0.035 [0.000, 5.000], mean action: 1.066 [0.000, 2.000], mean observation: 11.924 [0.000, 47.000], loss: 0.059982, mae: 2.026929, mean_q: 3.126138\n",
      "  4361/50000: episode: 19, duration: 1.087s, episode steps: 151, steps per second: 139, episode reward: 5.000, mean reward: 0.033 [0.000, 5.000], mean action: 1.113 [0.000, 2.000], mean observation: 13.373 [2.000, 47.000], loss: 0.078607, mae: 2.325291, mean_q: 3.566720\n",
      "  4772/50000: episode: 20, duration: 2.923s, episode steps: 411, steps per second: 141, episode reward: 15.000, mean reward: 0.036 [0.000, 5.000], mean action: 1.075 [0.000, 2.000], mean observation: 12.556 [0.000, 47.000], loss: 0.067027, mae: 2.596461, mean_q: 3.988421\n"
     ],
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": [
      "/home/marcofavorito/.virtualenvs/gym-breakout-pygame-7UQzWS9l/lib/python3.7/site-packages/rl/memory.py:39: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# Configure and compile the RL agent\n",
    "memory = SequentialMemory(limit=50000, window_length=1)\n",
    "policy = BoltzmannQPolicy()\n",
    "dqn = DQNAgent(model=model, nb_actions=nb_actions, memory=memory, nb_steps_warmup=10,\n",
    "               target_model_update=1e-2, policy=policy)\n",
    "dqn.compile(Adam(lr=1e-3), metrics=['mae'])\n",
    "\n",
    "# learn\n",
    "dqn.fit(env, nb_steps=50000, visualize=False, verbose=2)\n",
    "\n",
    "# save \n",
    "dqn.save_weights('dqn_{}_weights.h5f'.format(\"breakout-n\"), overwrite=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Evaluate for 5 episodes.\n",
    "dqn.test(Monitor(env, \".\", force=True), nb_episodes=5, visualize=False)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%% md  \n"
    }
   },
   "source": [
    "\n",
    "Now check the `examples/` folder, you should be able to see the recordings of the learned policy.\n"
   ],
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}