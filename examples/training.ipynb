{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Learn to play at Breakout \n",
    "\n",
    "### Requirements\n",
    "\n",
    "- In the repo root directory, do `pipenv install --dev` \n",
    "- Or, install the needed packages:\n",
    "\n",
    "      pip install keras-rl gym_breakout_pygame keras tensorflow-cpu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%import numpy as np\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "pygame 1.9.6\nHello from the pygame community. https://www.pygame.org/contribute.html\n"
     ],
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": [
      "Using TensorFlow backend.\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "from gym.wrappers import Monitor\n",
    "from gym_breakout_pygame.wrappers.normal_space import BreakoutNMultiDiscrete\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.policy import BoltzmannQPolicy\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nflatten_2 (Flatten)          (None, 4)                 0         \n_________________________________________________________________\ndense_4 (Dense)              (None, 64)                320       \n_________________________________________________________________\nactivation_4 (Activation)    (None, 64)                0         \n_________________________________________________________________\ndense_5 (Dense)              (None, 64)                4160      \n_________________________________________________________________\nactivation_5 (Activation)    (None, 64)                0         \n_________________________________________________________________\ndense_6 (Dense)              (None, 3)                 195       \n_________________________________________________________________\nactivation_6 (Activation)    (None, 3)                 0         \n=================================================================\nTotal params: 4,675\nTrainable params: 4,675\nNon-trainable params: 0\n_________________________________________________________________\nNone\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "env = BreakoutNMultiDiscrete()\n",
    "np.random.seed(123)\n",
    "env.seed(123)\n",
    "nb_actions = env.action_space.n\n",
    "\n",
    "# Next, we build a very simple model.\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(nb_actions))\n",
    "model.add(Activation('linear'))\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Training for 40000 steps ...\n",
      "   151/40000: episode: 1, duration: 1.333s, episode steps: 151, steps per second: 113, episode reward: 5.000, mean reward: 0.033 [0.000, 5.000], mean action: 1.013 [0.000, 2.000], mean observation: 12.293 [0.000, 47.000], loss: 0.028942, mean_absolute_error: 6.766470, mean_q: 10.180932\n",
      "   570/40000: episode: 2, duration: 1.926s, episode steps: 419, steps per second: 218, episode reward: 15.000, mean reward: 0.036 [0.000, 5.000], mean action: 1.086 [0.000, 2.000], mean observation: 11.298 [0.000, 47.000], loss: 0.070822, mean_absolute_error: 6.393366, mean_q: 9.582656\n",
      "   851/40000: episode: 3, duration: 1.333s, episode steps: 281, steps per second: 211, episode reward: 10.000, mean reward: 0.036 [0.000, 5.000], mean action: 0.961 [0.000, 2.000], mean observation: 10.996 [0.000, 47.000], loss: 0.082614, mean_absolute_error: 6.643369, mean_q: 9.987751\n",
      "  1002/40000: episode: 4, duration: 0.697s, episode steps: 151, steps per second: 217, episode reward: 5.000, mean reward: 0.033 [0.000, 5.000], mean action: 1.106 [0.000, 2.000], mean observation: 13.167 [2.000, 47.000], loss: 0.074375, mean_absolute_error: 6.613399, mean_q: 9.924868\n",
      "  1721/40000: episode: 5, duration: 3.415s, episode steps: 719, steps per second: 211, episode reward: 35.000, mean reward: 0.049 [0.000, 5.000], mean action: 1.051 [0.000, 2.000], mean observation: 10.991 [0.000, 47.000], loss: 0.086763, mean_absolute_error: 6.511625, mean_q: 9.776088\n",
      "  1872/40000: episode: 6, duration: 0.757s, episode steps: 151, steps per second: 199, episode reward: 5.000, mean reward: 0.033 [0.000, 5.000], mean action: 0.967 [0.000, 2.000], mean observation: 12.058 [0.000, 47.000], loss: 0.097993, mean_absolute_error: 6.552708, mean_q: 9.834184\n",
      "  2291/40000: episode: 7, duration: 2.027s, episode steps: 419, steps per second: 207, episode reward: 15.000, mean reward: 0.036 [0.000, 5.000], mean action: 1.074 [0.000, 2.000], mean observation: 11.737 [0.000, 47.000], loss: 0.093950, mean_absolute_error: 6.587142, mean_q: 9.883956\n",
      "  2442/40000: episode: 8, duration: 0.692s, episode steps: 151, steps per second: 218, episode reward: 5.000, mean reward: 0.033 [0.000, 5.000], mean action: 1.007 [0.000, 2.000], mean observation: 12.359 [1.000, 47.000], loss: 0.181227, mean_absolute_error: 6.557155, mean_q: 9.825447\n",
      "  2733/40000: episode: 9, duration: 1.393s, episode steps: 291, steps per second: 209, episode reward: 10.000, mean reward: 0.034 [0.000, 5.000], mean action: 1.017 [0.000, 2.000], mean observation: 11.678 [0.000, 47.000], loss: 0.150033, mean_absolute_error: 6.615955, mean_q: 9.923316\n",
      "  3284/40000: episode: 10, duration: 2.645s, episode steps: 551, steps per second: 208, episode reward: 20.000, mean reward: 0.036 [0.000, 5.000], mean action: 0.985 [0.000, 2.000], mean observation: 11.108 [0.000, 47.000], loss: 0.113878, mean_absolute_error: 6.589596, mean_q: 9.887782\n",
      "  3435/40000: episode: 11, duration: 0.696s, episode steps: 151, steps per second: 217, episode reward: 5.000, mean reward: 0.033 [0.000, 5.000], mean action: 0.980 [0.000, 2.000], mean observation: 13.109 [2.000, 47.000], loss: 0.090889, mean_absolute_error: 6.551699, mean_q: 9.827841\n",
      "  3854/40000: episode: 12, duration: 2.072s, episode steps: 419, steps per second: 202, episode reward: 15.000, mean reward: 0.036 [0.000, 5.000], mean action: 0.938 [0.000, 2.000], mean observation: 11.449 [0.000, 47.000], loss: 0.121988, mean_absolute_error: 6.581302, mean_q: 9.872912\n",
      "  4135/40000: episode: 13, duration: 1.498s, episode steps: 281, steps per second: 188, episode reward: 10.000, mean reward: 0.036 [0.000, 5.000], mean action: 1.007 [0.000, 2.000], mean observation: 11.338 [0.000, 47.000], loss: 0.124376, mean_absolute_error: 6.544605, mean_q: 9.812502\n",
      "  4416/40000: episode: 14, duration: 1.513s, episode steps: 281, steps per second: 186, episode reward: 10.000, mean reward: 0.036 [0.000, 5.000], mean action: 1.043 [0.000, 2.000], mean observation: 11.147 [0.000, 47.000], loss: 0.119411, mean_absolute_error: 6.530137, mean_q: 9.797074\n",
      "  4697/40000: episode: 15, duration: 1.604s, episode steps: 281, steps per second: 175, episode reward: 10.000, mean reward: 0.036 [0.000, 5.000], mean action: 1.007 [0.000, 2.000], mean observation: 11.965 [0.000, 47.000], loss: 0.111212, mean_absolute_error: 6.512589, mean_q: 9.769899\n  4718/40000: episode: 16, duration: 0.108s, episode steps: 21, steps per second: 194, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.286 [0.000, 2.000], mean observation: 16.488 [1.000, 47.000], loss: 0.120422, mean_absolute_error: 6.375947, mean_q: 9.551839\n",
      "  5129/40000: episode: 17, duration: 1.984s, episode steps: 411, steps per second: 207, episode reward: 15.000, mean reward: 0.036 [0.000, 5.000], mean action: 1.075 [0.000, 2.000], mean observation: 11.523 [0.000, 47.000], loss: 0.124823, mean_absolute_error: 6.463005, mean_q: 9.676118\n",
      "  5572/40000: episode: 18, duration: 2.195s, episode steps: 443, steps per second: 202, episode reward: 25.000, mean reward: 0.056 [0.000, 5.000], mean action: 0.995 [0.000, 2.000], mean observation: 10.680 [0.000, 47.000], loss: 0.129489, mean_absolute_error: 6.428757, mean_q: 9.628479\n",
      "  6504/40000: episode: 19, duration: 4.813s, episode steps: 932, steps per second: 194, episode reward: 35.000, mean reward: 0.038 [0.000, 5.000], mean action: 1.003 [0.000, 2.000], mean observation: 10.231 [0.000, 47.000], loss: 0.122122, mean_absolute_error: 6.451939, mean_q: 9.667704\n",
      "  6915/40000: episode: 20, duration: 1.953s, episode steps: 411, steps per second: 210, episode reward: 15.000, mean reward: 0.036 [0.000, 5.000], mean action: 1.017 [0.000, 2.000], mean observation: 11.414 [0.000, 47.000], loss: 0.161719, mean_absolute_error: 6.477230, mean_q: 9.703837\n",
      "  7326/40000: episode: 21, duration: 2.023s, episode steps: 411, steps per second: 203, episode reward: 15.000, mean reward: 0.036 [0.000, 5.000], mean action: 1.010 [0.000, 2.000], mean observation: 11.171 [0.000, 47.000], loss: 0.141398, mean_absolute_error: 6.465643, mean_q: 9.679949\n",
      "  7607/40000: episode: 22, duration: 1.431s, episode steps: 281, steps per second: 196, episode reward: 10.000, mean reward: 0.036 [0.000, 5.000], mean action: 0.961 [0.000, 2.000], mean observation: 10.958 [0.000, 47.000], loss: 0.117600, mean_absolute_error: 6.461000, mean_q: 9.673695\n",
      "  7888/40000: episode: 23, duration: 1.405s, episode steps: 281, steps per second: 200, episode reward: 10.000, mean reward: 0.036 [0.000, 5.000], mean action: 0.996 [0.000, 2.000], mean observation: 11.336 [0.000, 47.000], loss: 0.126676, mean_absolute_error: 6.415624, mean_q: 9.604650\n",
      "  9186/40000: episode: 24, duration: 6.652s, episode steps: 1298, steps per second: 195, episode reward: 40.000, mean reward: 0.031 [0.000, 5.000], mean action: 0.959 [0.000, 2.000], mean observation: 10.660 [0.000, 47.000], loss: 0.130845, mean_absolute_error: 6.403685, mean_q: 9.593999\n",
      "  9469/40000: episode: 25, duration: 1.465s, episode steps: 283, steps per second: 193, episode reward: 10.000, mean reward: 0.035 [0.000, 5.000], mean action: 0.979 [0.000, 2.000], mean observation: 11.064 [0.000, 47.000], loss: 0.132387, mean_absolute_error: 6.429589, mean_q: 9.618947\n",
      "  9896/40000: episode: 26, duration: 2.385s, episode steps: 427, steps per second: 179, episode reward: 15.000, mean reward: 0.035 [0.000, 5.000], mean action: 1.040 [0.000, 2.000], mean observation: 11.088 [0.000, 47.000], loss: 0.128176, mean_absolute_error: 6.406295, mean_q: 9.591038\n",
      " 10731/40000: episode: 27, duration: 4.574s, episode steps: 835, steps per second: 183, episode reward: 30.000, mean reward: 0.036 [0.000, 5.000], mean action: 1.046 [0.000, 2.000], mean observation: 10.727 [0.000, 47.000], loss: 0.142337, mean_absolute_error: 6.419077, mean_q: 9.608718\n",
      " 11016/40000: episode: 28, duration: 1.632s, episode steps: 285, steps per second: 175, episode reward: 10.000, mean reward: 0.035 [0.000, 5.000], mean action: 1.039 [0.000, 2.000], mean observation: 11.585 [0.000, 47.000], loss: 0.132983, mean_absolute_error: 6.459290, mean_q: 9.663899\n 11037/40000: episode: 29, duration: 0.130s, episode steps: 21, steps per second: 162, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.190 [0.000, 2.000], mean observation: 16.393 [1.000, 47.000], loss: 0.184714, mean_absolute_error: 6.455235, mean_q: 9.647337\n",
      " 11318/40000: episode: 30, duration: 1.692s, episode steps: 281, steps per second: 166, episode reward: 10.000, mean reward: 0.036 [0.000, 5.000], mean action: 0.979 [0.000, 2.000], mean observation: 11.605 [0.000, 47.000], loss: 0.130985, mean_absolute_error: 6.407640, mean_q: 9.591849\n",
      " 12021/40000: episode: 31, duration: 3.798s, episode steps: 703, steps per second: 185, episode reward: 25.000, mean reward: 0.036 [0.000, 5.000], mean action: 1.070 [0.000, 2.000], mean observation: 10.874 [0.000, 47.000], loss: 0.138521, mean_absolute_error: 6.378328, mean_q: 9.548338\n",
      " 12172/40000: episode: 32, duration: 0.855s, episode steps: 151, steps per second: 177, episode reward: 5.000, mean reward: 0.033 [0.000, 5.000], mean action: 0.940 [0.000, 2.000], mean observation: 12.598 [1.000, 47.000], loss: 0.094827, mean_absolute_error: 6.377045, mean_q: 9.549920\n",
      " 12459/40000: episode: 33, duration: 1.602s, episode steps: 287, steps per second: 179, episode reward: 10.000, mean reward: 0.035 [0.000, 5.000], mean action: 1.017 [0.000, 2.000], mean observation: 12.347 [0.000, 47.000], loss: 0.140934, mean_absolute_error: 6.365808, mean_q: 9.536434\n",
      " 12740/40000: episode: 34, duration: 1.680s, episode steps: 281, steps per second: 167, episode reward: 10.000, mean reward: 0.036 [0.000, 5.000], mean action: 1.021 [0.000, 2.000], mean observation: 11.739 [0.000, 47.000], loss: 0.124866, mean_absolute_error: 6.313312, mean_q: 9.446392\n",
      " 13021/40000: episode: 35, duration: 1.707s, episode steps: 281, steps per second: 165, episode reward: 10.000, mean reward: 0.036 [0.000, 5.000], mean action: 0.940 [0.000, 2.000], mean observation: 11.725 [0.000, 47.000], loss: 0.121281, mean_absolute_error: 6.253001, mean_q: 9.358481\n",
      " 13172/40000: episode: 36, duration: 0.860s, episode steps: 151, steps per second: 176, episode reward: 5.000, mean reward: 0.033 [0.000, 5.000], mean action: 1.000 [0.000, 2.000], mean observation: 12.972 [2.000, 47.000], loss: 0.108450, mean_absolute_error: 6.239463, mean_q: 9.334211\n",
      " 13463/40000: episode: 37, duration: 1.684s, episode steps: 291, steps per second: 173, episode reward: 10.000, mean reward: 0.034 [0.000, 5.000], mean action: 1.079 [0.000, 2.000], mean observation: 11.985 [0.000, 47.000], loss: 0.105947, mean_absolute_error: 6.202618, mean_q: 9.281925\n",
      " 13616/40000: episode: 38, duration: 0.917s, episode steps: 153, steps per second: 167, episode reward: 5.000, mean reward: 0.033 [0.000, 5.000], mean action: 1.072 [0.000, 2.000], mean observation: 12.173 [0.000, 47.000], loss: 0.123122, mean_absolute_error: 6.211947, mean_q: 9.289166\n",
      " 14167/40000: episode: 39, duration: 3.241s, episode steps: 551, steps per second: 170, episode reward: 20.000, mean reward: 0.036 [0.000, 5.000], mean action: 1.044 [0.000, 2.000], mean observation: 11.863 [0.000, 47.000], loss: 0.123520, mean_absolute_error: 6.135351, mean_q: 9.175069\n",
      " 14318/40000: episode: 40, duration: 0.967s, episode steps: 151, steps per second: 156, episode reward: 5.000, mean reward: 0.033 [0.000, 5.000], mean action: 1.026 [0.000, 2.000], mean observation: 12.058 [0.000, 47.000], loss: 0.090230, mean_absolute_error: 6.056825, mean_q: 9.057127\n",
      " 14469/40000: episode: 41, duration: 1.049s, episode steps: 151, steps per second: 144, episode reward: 5.000, mean reward: 0.033 [0.000, 5.000], mean action: 0.947 [0.000, 2.000], mean observation: 13.040 [2.000, 47.000], loss: 0.127093, mean_absolute_error: 6.010303, mean_q: 8.985974\n",
      " 14750/40000: episode: 42, duration: 1.811s, episode steps: 281, steps per second: 155, episode reward: 10.000, mean reward: 0.036 [0.000, 5.000], mean action: 1.032 [0.000, 2.000], mean observation: 11.335 [0.000, 47.000], loss: 0.099009, mean_absolute_error: 5.997391, mean_q: 8.969049\n",
      " 14901/40000: episode: 43, duration: 0.912s, episode steps: 151, steps per second: 166, episode reward: 5.000, mean reward: 0.033 [0.000, 5.000], mean action: 1.053 [0.000, 2.000], mean observation: 12.257 [0.000, 47.000], loss: 0.102998, mean_absolute_error: 5.941644, mean_q: 8.883739\n",
      " 15052/40000: episode: 44, duration: 0.863s, episode steps: 151, steps per second: 175, episode reward: 5.000, mean reward: 0.033 [0.000, 5.000], mean action: 0.967 [0.000, 2.000], mean observation: 11.720 [0.000, 47.000], loss: 0.085639, mean_absolute_error: 5.898145, mean_q: 8.822670\n",
      " 15463/40000: episode: 45, duration: 2.398s, episode steps: 411, steps per second: 171, episode reward: 15.000, mean reward: 0.036 [0.000, 5.000], mean action: 0.939 [0.000, 2.000], mean observation: 11.380 [0.000, 47.000], loss: 0.103382, mean_absolute_error: 5.843539, mean_q: 8.740027\n",
      " 15614/40000: episode: 46, duration: 0.938s, episode steps: 151, steps per second: 161, episode reward: 5.000, mean reward: 0.033 [0.000, 5.000], mean action: 1.040 [0.000, 2.000], mean observation: 11.495 [0.000, 47.000], loss: 0.117804, mean_absolute_error: 5.850436, mean_q: 8.754273\n",
      " 16041/40000: episode: 47, duration: 2.550s, episode steps: 427, steps per second: 167, episode reward: 15.000, mean reward: 0.035 [0.000, 5.000], mean action: 1.030 [0.000, 2.000], mean observation: 12.220 [0.000, 47.000], loss: 0.090912, mean_absolute_error: 5.758710, mean_q: 8.611487\n",
      " 16192/40000: episode: 48, duration: 0.887s, episode steps: 151, steps per second: 170, episode reward: 5.000, mean reward: 0.033 [0.000, 5.000], mean action: 0.993 [0.000, 2.000], mean observation: 12.969 [2.000, 47.000], loss: 0.082387, mean_absolute_error: 5.742327, mean_q: 8.582506\n",
      " 16607/40000: episode: 49, duration: 2.383s, episode steps: 415, steps per second: 174, episode reward: 15.000, mean reward: 0.036 [0.000, 5.000], mean action: 1.005 [0.000, 2.000], mean observation: 11.589 [0.000, 47.000], loss: 0.101338, mean_absolute_error: 5.653841, mean_q: 8.456060\n",
      " 16896/40000: episode: 50, duration: 1.708s, episode steps: 289, steps per second: 169, episode reward: 10.000, mean reward: 0.035 [0.000, 5.000], mean action: 1.007 [0.000, 2.000], mean observation: 11.925 [0.000, 47.000], loss: 0.077608, mean_absolute_error: 5.617969, mean_q: 8.401060\n",
      " 17177/40000: episode: 51, duration: 1.645s, episode steps: 281, steps per second: 171, episode reward: 10.000, mean reward: 0.036 [0.000, 5.000], mean action: 1.021 [0.000, 2.000], mean observation: 11.828 [0.000, 47.000], loss: 0.081630, mean_absolute_error: 5.548061, mean_q: 8.305309\n",
      " 17596/40000: episode: 52, duration: 2.397s, episode steps: 419, steps per second: 175, episode reward: 15.000, mean reward: 0.036 [0.000, 5.000], mean action: 0.928 [0.000, 2.000], mean observation: 10.986 [0.000, 47.000], loss: 0.066861, mean_absolute_error: 5.479704, mean_q: 8.202082\n",
      " 17747/40000: episode: 53, duration: 1.033s, episode steps: 151, steps per second: 146, episode reward: 5.000, mean reward: 0.033 [0.000, 5.000], mean action: 1.113 [0.000, 2.000], mean observation: 13.235 [2.000, 47.000], loss: 0.069825, mean_absolute_error: 5.448063, mean_q: 8.147873\n",
      " 17898/40000: episode: 54, duration: 0.955s, episode steps: 151, steps per second: 158, episode reward: 5.000, mean reward: 0.033 [0.000, 5.000], mean action: 0.940 [0.000, 2.000], mean observation: 13.172 [2.000, 47.000], loss: 0.070653, mean_absolute_error: 5.421359, mean_q: 8.111514\n",
      " 18589/40000: episode: 55, duration: 3.965s, episode steps: 691, steps per second: 174, episode reward: 30.000, mean reward: 0.043 [0.000, 5.000], mean action: 0.990 [0.000, 2.000], mean observation: 10.022 [0.000, 47.000], loss: 0.077641, mean_absolute_error: 5.345649, mean_q: 7.996246\n",
      " 19008/40000: episode: 56, duration: 2.068s, episode steps: 419, steps per second: 203, episode reward: 15.000, mean reward: 0.036 [0.000, 5.000], mean action: 0.943 [0.000, 2.000], mean observation: 11.181 [0.000, 47.000], loss: 0.070575, mean_absolute_error: 5.285333, mean_q: 7.912545\n",
      " 19159/40000: episode: 57, duration: 0.720s, episode steps: 151, steps per second: 210, episode reward: 5.000, mean reward: 0.033 [0.000, 5.000], mean action: 0.947 [0.000, 2.000], mean observation: 11.828 [0.000, 47.000], loss: 0.055733, mean_absolute_error: 5.259486, mean_q: 7.864049\n",
      " 19314/40000: episode: 58, duration: 0.749s, episode steps: 155, steps per second: 207, episode reward: 5.000, mean reward: 0.032 [0.000, 5.000], mean action: 1.103 [0.000, 2.000], mean observation: 11.926 [0.000, 47.000], loss: 0.075813, mean_absolute_error: 5.246635, mean_q: 7.840355\n",
      " 19871/40000: episode: 59, duration: 2.703s, episode steps: 557, steps per second: 206, episode reward: 20.000, mean reward: 0.036 [0.000, 5.000], mean action: 1.032 [0.000, 2.000], mean observation: 10.796 [0.000, 47.000], loss: 0.080551, mean_absolute_error: 5.210493, mean_q: 7.795837\n",
      " 20022/40000: episode: 60, duration: 0.737s, episode steps: 151, steps per second: 205, episode reward: 5.000, mean reward: 0.033 [0.000, 5.000], mean action: 1.086 [0.000, 2.000], mean observation: 12.674 [1.000, 47.000], loss: 0.059252, mean_absolute_error: 5.166071, mean_q: 7.736216\n",
      " 20173/40000: episode: 61, duration: 0.821s, episode steps: 151, steps per second: 184, episode reward: 5.000, mean reward: 0.033 [0.000, 5.000], mean action: 1.053 [0.000, 2.000], mean observation: 12.071 [0.000, 47.000], loss: 0.072621, mean_absolute_error: 5.159980, mean_q: 7.722809\n",
      " 20324/40000: episode: 62, duration: 0.728s, episode steps: 151, steps per second: 208, episode reward: 5.000, mean reward: 0.033 [0.000, 5.000], mean action: 0.914 [0.000, 2.000], mean observation: 11.627 [0.000, 47.000], loss: 0.084847, mean_absolute_error: 5.164638, mean_q: 7.717853\n",
      " 20475/40000: episode: 63, duration: 0.749s, episode steps: 151, steps per second: 202, episode reward: 5.000, mean reward: 0.033 [0.000, 5.000], mean action: 0.914 [0.000, 2.000], mean observation: 11.588 [0.000, 47.000], loss: 0.091933, mean_absolute_error: 5.152504, mean_q: 7.696326\n",
      " 21310/40000: episode: 64, duration: 4.246s, episode steps: 835, steps per second: 197, episode reward: 30.000, mean reward: 0.036 [0.000, 5.000], mean action: 0.996 [0.000, 2.000], mean observation: 10.823 [0.000, 47.000], loss: 0.074153, mean_absolute_error: 5.066115, mean_q: 7.578413\n",
      " 21461/40000: episode: 65, duration: 0.835s, episode steps: 151, steps per second: 181, episode reward: 5.000, mean reward: 0.033 [0.000, 5.000], mean action: 0.967 [0.000, 2.000], mean observation: 12.086 [0.000, 47.000], loss: 0.051015, mean_absolute_error: 5.021775, mean_q: 7.516516\n",
      " 21742/40000: episode: 66, duration: 1.385s, episode steps: 281, steps per second: 203, episode reward: 10.000, mean reward: 0.036 [0.000, 5.000], mean action: 1.057 [0.000, 2.000], mean observation: 11.852 [0.000, 47.000], loss: 0.051556, mean_absolute_error: 5.018182, mean_q: 7.512294\n",
      " 22161/40000: episode: 67, duration: 1.999s, episode steps: 419, steps per second: 210, episode reward: 15.000, mean reward: 0.036 [0.000, 5.000], mean action: 0.936 [0.000, 2.000], mean observation: 11.240 [0.000, 47.000], loss: 0.073481, mean_absolute_error: 4.983219, mean_q: 7.453872\n",
      " 22314/40000: episode: 68, duration: 0.800s, episode steps: 153, steps per second: 191, episode reward: 5.000, mean reward: 0.033 [0.000, 5.000], mean action: 1.033 [0.000, 2.000], mean observation: 11.454 [0.000, 47.000], loss: 0.060599, mean_absolute_error: 4.942508, mean_q: 7.398329\n",
      " 22595/40000: episode: 69, duration: 1.349s, episode steps: 281, steps per second: 208, episode reward: 10.000, mean reward: 0.036 [0.000, 5.000], mean action: 1.025 [0.000, 2.000], mean observation: 11.038 [0.000, 47.000], loss: 0.056537, mean_absolute_error: 4.924321, mean_q: 7.368438\n",
      " 22746/40000: episode: 70, duration: 0.788s, episode steps: 151, steps per second: 192, episode reward: 5.000, mean reward: 0.033 [0.000, 5.000], mean action: 0.987 [0.000, 2.000], mean observation: 11.488 [0.000, 47.000], loss: 0.077504, mean_absolute_error: 4.898509, mean_q: 7.320955\n",
      " 23027/40000: episode: 71, duration: 1.530s, episode steps: 281, steps per second: 184, episode reward: 10.000, mean reward: 0.036 [0.000, 5.000], mean action: 1.068 [0.000, 2.000], mean observation: 12.246 [0.000, 47.000], loss: 0.061675, mean_absolute_error: 4.897053, mean_q: 7.324489\n",
      " 23178/40000: episode: 72, duration: 0.788s, episode steps: 151, steps per second: 192, episode reward: 5.000, mean reward: 0.033 [0.000, 5.000], mean action: 1.119 [0.000, 2.000], mean observation: 13.325 [2.000, 47.000], loss: 0.066956, mean_absolute_error: 4.826269, mean_q: 7.217119\n",
      " 23463/40000: episode: 73, duration: 1.791s, episode steps: 285, steps per second: 159, episode reward: 10.000, mean reward: 0.035 [0.000, 5.000], mean action: 0.958 [0.000, 2.000], mean observation: 11.413 [0.000, 47.000], loss: 0.053862, mean_absolute_error: 4.835807, mean_q: 7.234929\n",
      " 23744/40000: episode: 74, duration: 1.625s, episode steps: 281, steps per second: 173, episode reward: 10.000, mean reward: 0.036 [0.000, 5.000], mean action: 1.025 [0.000, 2.000], mean observation: 11.471 [0.000, 47.000], loss: 0.054349, mean_absolute_error: 4.797645, mean_q: 7.180246\n",
      " 23895/40000: episode: 75, duration: 0.816s, episode steps: 151, steps per second: 185, episode reward: 5.000, mean reward: 0.033 [0.000, 5.000], mean action: 1.007 [0.000, 2.000], mean observation: 11.310 [0.000, 47.000], loss: 0.060222, mean_absolute_error: 4.862445, mean_q: 7.275457\n",
      " 24046/40000: episode: 76, duration: 0.793s, episode steps: 151, steps per second: 190, episode reward: 5.000, mean reward: 0.033 [0.000, 5.000], mean action: 0.868 [0.000, 2.000], mean observation: 12.454 [0.000, 47.000], loss: 0.062756, mean_absolute_error: 4.805647, mean_q: 7.180601\n",
      " 24759/40000: episode: 77, duration: 3.795s, episode steps: 713, steps per second: 188, episode reward: 35.000, mean reward: 0.049 [0.000, 5.000], mean action: 0.989 [0.000, 2.000], mean observation: 11.042 [0.000, 47.000], loss: 0.055775, mean_absolute_error: 4.815273, mean_q: 7.210506\n",
      " 25446/40000: episode: 78, duration: 3.497s, episode steps: 687, steps per second: 196, episode reward: 25.000, mean reward: 0.036 [0.000, 5.000], mean action: 1.006 [0.000, 2.000], mean observation: 11.399 [0.000, 47.000], loss: 0.072782, mean_absolute_error: 4.771113, mean_q: 7.138614\n",
      " 25735/40000: episode: 79, duration: 1.820s, episode steps: 289, steps per second: 159, episode reward: 10.000, mean reward: 0.035 [0.000, 5.000], mean action: 0.986 [0.000, 2.000], mean observation: 11.646 [0.000, 47.000], loss: 0.062534, mean_absolute_error: 4.758770, mean_q: 7.121859\n",
      " 25886/40000: episode: 80, duration: 0.806s, episode steps: 151, steps per second: 187, episode reward: 5.000, mean reward: 0.033 [0.000, 5.000], mean action: 1.013 [0.000, 2.000], mean observation: 12.626 [0.000, 47.000], loss: 0.053088, mean_absolute_error: 4.701646, mean_q: 7.035068\n",
      " 26037/40000: episode: 81, duration: 0.797s, episode steps: 151, steps per second: 190, episode reward: 5.000, mean reward: 0.033 [0.000, 5.000], mean action: 0.874 [0.000, 2.000], mean observation: 12.300 [0.000, 47.000], loss: 0.053564, mean_absolute_error: 4.708528, mean_q: 7.051936\n",
      " 27034/40000: episode: 82, duration: 4.737s, episode steps: 997, steps per second: 210, episode reward: 45.000, mean reward: 0.045 [0.000, 5.000], mean action: 1.020 [0.000, 2.000], mean observation: 9.529 [0.000, 45.000], loss: 0.055555, mean_absolute_error: 4.701631, mean_q: 7.036469\n 27055/40000: episode: 83, duration: 0.103s, episode steps: 21, steps per second: 205, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.143 [0.000, 2.000], mean observation: 16.476 [1.000, 47.000], loss: 0.049955, mean_absolute_error: 4.703011, mean_q: 7.028466\n",
      " 27880/40000: episode: 84, duration: 4.008s, episode steps: 825, steps per second: 206, episode reward: 30.000, mean reward: 0.036 [0.000, 5.000], mean action: 0.983 [0.000, 2.000], mean observation: 11.207 [0.000, 47.000], loss: 0.066815, mean_absolute_error: 4.655071, mean_q: 6.958571\n",
      " 28033/40000: episode: 85, duration: 0.902s, episode steps: 153, steps per second: 170, episode reward: 5.000, mean reward: 0.033 [0.000, 5.000], mean action: 1.039 [0.000, 2.000], mean observation: 11.629 [0.000, 47.000], loss: 0.043284, mean_absolute_error: 4.589186, mean_q: 6.868907\n",
      " 28184/40000: episode: 86, duration: 0.820s, episode steps: 151, steps per second: 184, episode reward: 5.000, mean reward: 0.033 [0.000, 5.000], mean action: 1.046 [0.000, 2.000], mean observation: 11.699 [0.000, 47.000], loss: 0.054017, mean_absolute_error: 4.604764, mean_q: 6.887563\n",
      " 28741/40000: episode: 87, duration: 2.853s, episode steps: 557, steps per second: 195, episode reward: 20.000, mean reward: 0.036 [0.000, 5.000], mean action: 1.041 [0.000, 2.000], mean observation: 11.615 [0.000, 47.000], loss: 0.059742, mean_absolute_error: 4.557761, mean_q: 6.814109\n",
      " 28892/40000: episode: 88, duration: 0.764s, episode steps: 151, steps per second: 198, episode reward: 5.000, mean reward: 0.033 [0.000, 5.000], mean action: 0.901 [0.000, 2.000], mean observation: 12.033 [0.000, 47.000], loss: 0.063550, mean_absolute_error: 4.493191, mean_q: 6.718743\n",
      " 29049/40000: episode: 89, duration: 0.789s, episode steps: 157, steps per second: 199, episode reward: 5.000, mean reward: 0.032 [0.000, 5.000], mean action: 0.943 [0.000, 2.000], mean observation: 12.360 [0.000, 47.000], loss: 0.046621, mean_absolute_error: 4.502846, mean_q: 6.737445\n",
      " 29204/40000: episode: 90, duration: 0.749s, episode steps: 155, steps per second: 207, episode reward: 5.000, mean reward: 0.032 [0.000, 5.000], mean action: 1.116 [0.000, 2.000], mean observation: 12.379 [0.000, 47.000], loss: 0.060537, mean_absolute_error: 4.544648, mean_q: 6.798323\n",
      " 29355/40000: episode: 91, duration: 0.776s, episode steps: 151, steps per second: 194, episode reward: 5.000, mean reward: 0.033 [0.000, 5.000], mean action: 1.066 [0.000, 2.000], mean observation: 13.121 [2.000, 47.000], loss: 0.049437, mean_absolute_error: 4.440006, mean_q: 6.637125\n",
      " 29506/40000: episode: 92, duration: 0.740s, episode steps: 151, steps per second: 204, episode reward: 5.000, mean reward: 0.033 [0.000, 5.000], mean action: 0.987 [0.000, 2.000], mean observation: 13.051 [2.000, 47.000], loss: 0.054235, mean_absolute_error: 4.454581, mean_q: 6.660030\n",
      " 30205/40000: episode: 93, duration: 3.311s, episode steps: 699, steps per second: 211, episode reward: 25.000, mean reward: 0.036 [0.000, 5.000], mean action: 1.057 [0.000, 2.000], mean observation: 10.540 [0.000, 47.000], loss: 0.063356, mean_absolute_error: 4.433977, mean_q: 6.632185\n",
      " 30957/40000: episode: 94, duration: 3.835s, episode steps: 752, steps per second: 196, episode reward: 45.000, mean reward: 0.060 [0.000, 5.000], mean action: 0.985 [0.000, 2.000], mean observation: 10.102 [0.000, 45.000], loss: 0.056339, mean_absolute_error: 4.397826, mean_q: 6.580912\n",
      " 31813/40000: episode: 95, duration: 4.076s, episode steps: 856, steps per second: 210, episode reward: 40.000, mean reward: 0.047 [0.000, 5.000], mean action: 0.987 [0.000, 2.000], mean observation: 9.507 [0.000, 47.000], loss: 0.063948, mean_absolute_error: 4.404333, mean_q: 6.583785\n",
      " 31964/40000: episode: 96, duration: 0.724s, episode steps: 151, steps per second: 208, episode reward: 5.000, mean reward: 0.033 [0.000, 5.000], mean action: 0.901 [0.000, 2.000], mean observation: 12.858 [2.000, 47.000], loss: 0.062303, mean_absolute_error: 4.399868, mean_q: 6.572062\n",
      " 32245/40000: episode: 97, duration: 1.385s, episode steps: 281, steps per second: 203, episode reward: 10.000, mean reward: 0.036 [0.000, 5.000], mean action: 1.011 [0.000, 2.000], mean observation: 11.755 [0.000, 47.000], loss: 0.072171, mean_absolute_error: 4.392832, mean_q: 6.566248\n",
      " 32534/40000: episode: 98, duration: 1.405s, episode steps: 289, steps per second: 206, episode reward: 10.000, mean reward: 0.035 [0.000, 5.000], mean action: 0.934 [0.000, 2.000], mean observation: 11.554 [0.000, 47.000], loss: 0.063414, mean_absolute_error: 4.342381, mean_q: 6.484561\n",
      " 33264/40000: episode: 99, duration: 3.561s, episode steps: 730, steps per second: 205, episode reward: 35.000, mean reward: 0.048 [0.000, 5.000], mean action: 0.975 [0.000, 2.000], mean observation: 9.821 [0.000, 47.000], loss: 0.068372, mean_absolute_error: 4.348488, mean_q: 6.501615\n",
      " 33967/40000: episode: 100, duration: 3.393s, episode steps: 703, steps per second: 207, episode reward: 35.000, mean reward: 0.050 [0.000, 5.000], mean action: 1.014 [0.000, 2.000], mean observation: 11.524 [0.000, 47.000], loss: 0.061281, mean_absolute_error: 4.373428, mean_q: 6.541636\n",
      " 34118/40000: episode: 101, duration: 0.745s, episode steps: 151, steps per second: 203, episode reward: 5.000, mean reward: 0.033 [0.000, 5.000], mean action: 1.119 [0.000, 2.000], mean observation: 12.478 [0.000, 47.000], loss: 0.076358, mean_absolute_error: 4.373533, mean_q: 6.544819\n",
      " 34275/40000: episode: 102, duration: 0.849s, episode steps: 157, steps per second: 185, episode reward: 5.000, mean reward: 0.032 [0.000, 5.000], mean action: 1.025 [0.000, 2.000], mean observation: 12.387 [0.000, 47.000], loss: 0.066633, mean_absolute_error: 4.352865, mean_q: 6.508897\n",
      " 34426/40000: episode: 103, duration: 0.782s, episode steps: 151, steps per second: 193, episode reward: 5.000, mean reward: 0.033 [0.000, 5.000], mean action: 1.113 [0.000, 2.000], mean observation: 12.738 [1.000, 47.000], loss: 0.071202, mean_absolute_error: 4.376423, mean_q: 6.544353\n",
      " 34577/40000: episode: 104, duration: 0.800s, episode steps: 151, steps per second: 189, episode reward: 5.000, mean reward: 0.033 [0.000, 5.000], mean action: 1.040 [0.000, 2.000], mean observation: 12.217 [0.000, 47.000], loss: 0.065276, mean_absolute_error: 4.404625, mean_q: 6.582662\n",
      " 34728/40000: episode: 105, duration: 0.752s, episode steps: 151, steps per second: 201, episode reward: 5.000, mean reward: 0.033 [0.000, 5.000], mean action: 1.073 [0.000, 2.000], mean observation: 12.371 [2.000, 47.000], loss: 0.081963, mean_absolute_error: 4.394506, mean_q: 6.569979\n",
      " 35009/40000: episode: 106, duration: 1.392s, episode steps: 281, steps per second: 202, episode reward: 10.000, mean reward: 0.036 [0.000, 5.000], mean action: 1.004 [0.000, 2.000], mean observation: 11.891 [0.000, 47.000], loss: 0.077815, mean_absolute_error: 4.407253, mean_q: 6.590452\n",
      " 35444/40000: episode: 107, duration: 2.121s, episode steps: 435, steps per second: 205, episode reward: 15.000, mean reward: 0.034 [0.000, 5.000], mean action: 1.034 [0.000, 2.000], mean observation: 11.290 [0.000, 47.000], loss: 0.076348, mean_absolute_error: 4.423657, mean_q: 6.620786\n",
      " 35595/40000: episode: 108, duration: 0.718s, episode steps: 151, steps per second: 210, episode reward: 5.000, mean reward: 0.033 [0.000, 5.000], mean action: 0.980 [0.000, 2.000], mean observation: 12.056 [0.000, 47.000], loss: 0.087198, mean_absolute_error: 4.414306, mean_q: 6.605887\n",
      " 35876/40000: episode: 109, duration: 1.379s, episode steps: 281, steps per second: 204, episode reward: 10.000, mean reward: 0.036 [0.000, 5.000], mean action: 0.989 [0.000, 2.000], mean observation: 12.121 [0.000, 47.000], loss: 0.058960, mean_absolute_error: 4.429757, mean_q: 6.631474\n",
      " 36027/40000: episode: 110, duration: 0.752s, episode steps: 151, steps per second: 201, episode reward: 5.000, mean reward: 0.033 [0.000, 5.000], mean action: 0.960 [0.000, 2.000], mean observation: 12.386 [0.000, 47.000], loss: 0.062892, mean_absolute_error: 4.443395, mean_q: 6.662072\n",
      " 36178/40000: episode: 111, duration: 0.745s, episode steps: 151, steps per second: 203, episode reward: 5.000, mean reward: 0.033 [0.000, 5.000], mean action: 0.987 [0.000, 2.000], mean observation: 12.325 [0.000, 47.000], loss: 0.066824, mean_absolute_error: 4.431706, mean_q: 6.634446\n",
      " 36459/40000: episode: 112, duration: 1.387s, episode steps: 281, steps per second: 203, episode reward: 10.000, mean reward: 0.036 [0.000, 5.000], mean action: 1.060 [0.000, 2.000], mean observation: 12.328 [0.000, 47.000], loss: 0.079116, mean_absolute_error: 4.431149, mean_q: 6.631476\n",
      " 36610/40000: episode: 113, duration: 0.733s, episode steps: 151, steps per second: 206, episode reward: 5.000, mean reward: 0.033 [0.000, 5.000], mean action: 1.040 [0.000, 2.000], mean observation: 12.873 [2.000, 47.000], loss: 0.116656, mean_absolute_error: 4.466185, mean_q: 6.684070\n",
      " 36761/40000: episode: 114, duration: 0.747s, episode steps: 151, steps per second: 202, episode reward: 5.000, mean reward: 0.033 [0.000, 5.000], mean action: 1.172 [0.000, 2.000], mean observation: 12.364 [0.000, 47.000], loss: 0.123879, mean_absolute_error: 4.472592, mean_q: 6.686768\n",
      " 36912/40000: episode: 115, duration: 0.726s, episode steps: 151, steps per second: 208, episode reward: 5.000, mean reward: 0.033 [0.000, 5.000], mean action: 1.007 [0.000, 2.000], mean observation: 12.876 [2.000, 47.000], loss: 0.089581, mean_absolute_error: 4.482524, mean_q: 6.708686\n 36933/40000: episode: 116, duration: 0.102s, episode steps: 21, steps per second: 206, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 0.857 [0.000, 2.000], mean observation: 15.667 [2.000, 47.000], loss: 0.072038, mean_absolute_error: 4.562766, mean_q: 6.834550\n",
      " 37084/40000: episode: 117, duration: 0.744s, episode steps: 151, steps per second: 203, episode reward: 5.000, mean reward: 0.033 [0.000, 5.000], mean action: 0.967 [0.000, 2.000], mean observation: 11.561 [0.000, 47.000], loss: 0.091570, mean_absolute_error: 4.455902, mean_q: 6.670783\n",
      " 37923/40000: episode: 118, duration: 3.933s, episode steps: 839, steps per second: 213, episode reward: 30.000, mean reward: 0.036 [0.000, 5.000], mean action: 0.994 [0.000, 2.000], mean observation: 11.518 [0.000, 47.000], loss: 0.086785, mean_absolute_error: 4.527650, mean_q: 6.773652\n",
      " 38506/40000: episode: 119, duration: 2.732s, episode steps: 583, steps per second: 213, episode reward: 30.000, mean reward: 0.051 [0.000, 5.000], mean action: 1.034 [0.000, 2.000], mean observation: 11.628 [0.000, 47.000], loss: 0.092976, mean_absolute_error: 4.576390, mean_q: 6.851830\n",
      " 38787/40000: episode: 120, duration: 1.352s, episode steps: 281, steps per second: 208, episode reward: 10.000, mean reward: 0.036 [0.000, 5.000], mean action: 1.064 [0.000, 2.000], mean observation: 11.705 [0.000, 47.000], loss: 0.114915, mean_absolute_error: 4.595083, mean_q: 6.876798\n",
      " 39198/40000: episode: 121, duration: 1.948s, episode steps: 411, steps per second: 211, episode reward: 15.000, mean reward: 0.036 [0.000, 5.000], mean action: 1.036 [0.000, 2.000], mean observation: 12.185 [0.000, 47.000], loss: 0.112565, mean_absolute_error: 4.624290, mean_q: 6.913054\n",
      " 39479/40000: episode: 122, duration: 1.400s, episode steps: 281, steps per second: 201, episode reward: 10.000, mean reward: 0.036 [0.000, 5.000], mean action: 1.021 [0.000, 2.000], mean observation: 11.484 [0.000, 47.000], loss: 0.084761, mean_absolute_error: 4.603777, mean_q: 6.887587\n",
      " 39630/40000: episode: 123, duration: 0.729s, episode steps: 151, steps per second: 207, episode reward: 5.000, mean reward: 0.033 [0.000, 5.000], mean action: 1.093 [0.000, 2.000], mean observation: 13.243 [2.000, 47.000], loss: 0.100654, mean_absolute_error: 4.566471, mean_q: 6.825614\n",
      " 39913/40000: episode: 124, duration: 1.619s, episode steps: 283, steps per second: 175, episode reward: 10.000, mean reward: 0.035 [0.000, 5.000], mean action: 1.042 [0.000, 2.000], mean observation: 11.302 [0.000, 47.000], loss: 0.083198, mean_absolute_error: 4.578217, mean_q: 6.859913\n",
      "done, took 208.384 seconds\n"
     ],
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": [
      "/home/marcofavorito/.virtualenvs/gym-breakout-pygame-7UQzWS9l/lib/python3.7/site-packages/rl/memory.py:39: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# Configure and compile the RL agent\n",
    "memory = SequentialMemory(limit=50000, window_length=1)\n",
    "policy = BoltzmannQPolicy()\n",
    "dqn = DQNAgent(model=model, nb_actions=nb_actions, memory=memory, nb_steps_warmup=10,\n",
    "               target_model_update=1e-2, policy=policy)\n",
    "dqn.compile(Adam(lr=1e-3), metrics=['mae'])\n",
    "\n",
    "# learn\n",
    "dqn.fit(env, nb_steps=50000, visualize=False, verbose=2)\n",
    "\n",
    "# save \n",
    "dqn.save_weights('dqn_{}_weights.h5f'.format(\"breakout-n\"), overwrite=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Testing for 5 episodes ...\n"
     ],
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": [
      "/home/marcofavorito/.virtualenvs/gym-breakout-pygame-7UQzWS9l/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Trying to monitor an environment which has no 'spec' set. This usually means you did not create it via 'gym.make', and is recommended only for advanced users.\u001b[0m\n  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ],
     "output_type": "stream"
    },
    {
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-41845024a281>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Evaluate for 5 episodes.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdqn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMonitor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\".\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_episodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvisualize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.virtualenvs/gym-breakout-pygame-7UQzWS9l/lib/python3.7/site-packages/rl/core.py\u001b[0m in \u001b[0;36mtest\u001b[0;34m(self, env, nb_episodes, action_repetition, callbacks, visualize, nb_max_episode_steps, nb_max_start_steps, start_step_policy, verbose)\u001b[0m\n\u001b[1;32m    307\u001b[0m             \u001b[0;31m# Obtain the initial observation by resetting the environment.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_states\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 309\u001b[0;31m             \u001b[0mobservation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    310\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessor\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m                 \u001b[0mobservation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_observation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/gym-breakout-pygame-7UQzWS9l/lib/python3.7/site-packages/gym/wrappers/monitor.py\u001b[0m in \u001b[0;36mreset\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_before_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mobservation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_after_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobservation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/gym-breakout-pygame-7UQzWS9l/lib/python3.7/site-packages/gym/wrappers/monitor.py\u001b[0m in \u001b[0;36m_after_reset\u001b[0;34m(self, observation)\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats_recorder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mafter_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_video_recorder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;31m# Bump *after* all reset activity has finished\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/gym-breakout-pygame-7UQzWS9l/lib/python3.7/site-packages/gym/wrappers/monitor.py\u001b[0m in \u001b[0;36mreset_video_recorder\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    207\u001b[0m             \u001b[0menabled\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_video_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m         )\n\u001b[0;32m--> 209\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvideo_recorder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcapture_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_close_video_recorder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/gym-breakout-pygame-7UQzWS9l/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py\u001b[0m in \u001b[0;36mcapture_frame\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0mrender_mode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'ansi'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mansi_mode\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'rgb_array'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m         \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrender_mode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mframe\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workfolder/gym-breakout-pygame/gym_breakout_pygame/breakout_env.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    476\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPygameViewer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 478\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    479\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    480\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workfolder/gym-breakout-pygame/gym_breakout_pygame/breakout_env.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"human\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fill_screen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_draw_score_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_draw_last_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workfolder/gym-breakout-pygame/gym_breakout_pygame/breakout_env.py\u001b[0m in \u001b[0;36m_fill_screen\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fill_screen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscreen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwhite\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_draw_score_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31merror\u001b[0m: display Surface quit"
     ],
     "ename": "error",
     "evalue": "display Surface quit",
     "output_type": "error"
    }
   ],
   "source": [
    "\n",
    "# Evaluate for 5 episodes.\n",
    "dqn.test(Monitor(env, \".\", force=True), nb_episodes=5, visualize=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "Now check the `examples/` folder, you should be able to see the recordings of the learned policy.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n",
     "is_executing": false
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}